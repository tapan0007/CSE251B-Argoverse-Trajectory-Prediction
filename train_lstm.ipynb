{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ea9f615f-1564-401f-9a74-8ac98f9bbeab",
   "metadata": {
    "id": "ea9f615f-1564-401f-9a74-8ac98f9bbeab",
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "import pickle\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8845100d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4f04ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"../train/train\"\n",
    "# The glob module finds all the pathnames matching a specified pattern\n",
    "train_pkl_lst = glob(os.path.join(train_path, '*'))\n",
    "with open(train_pkl_lst[1], 'rb') as f:\n",
    "    training_sample = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44b0f51-0779-49dc-bcab-5c284039d6fd",
   "metadata": {
    "id": "f44b0f51-0779-49dc-bcab-5c284039d6fd"
   },
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "68fac99f-9f5e-4fb5-ba6c-e22cc6f8f8be",
   "metadata": {
    "id": "68fac99f-9f5e-4fb5-ba6c-e22cc6f8f8be"
   },
   "outputs": [],
   "source": [
    "class MyLSTM(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, n_layers):\n",
    "        super(MyLSTM, self).__init__()\n",
    "\n",
    "        # Defining some parameters\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        #Defining the layers\n",
    "        # RNN Layer\n",
    "        self.lstm = nn.LSTM(input_size, hidden_dim, n_layers, batch_first=True)   \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        #Initializing hidden state for first input using method defined below\n",
    "        h_t, c_t = self.init_hidden(batch_size)\n",
    "\n",
    "        #print(x.size())\n",
    "        #print(x.size(-1))\n",
    "        # Passing in the input and hidden state into the model and obtaining outputs\n",
    "        out, (h_t, c_t) = self.lstm(x, (h_t, c_t))\n",
    "        \n",
    "        \n",
    "        # Reshaping the outputs such that it can be fit into the fully connected layer\n",
    "        out = out.contiguous().view(-1, self.hidden_dim)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out, h_t\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        # This method generates the first hidden state of zeros which we'll use in the forward pass\n",
    "        h_0 = torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(device)\n",
    "        c_0 =  torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(device)       \n",
    "         # We'll send the tensor holding the hidden state to the device we specified earlier as well\n",
    "        return h_0, c_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43901b7a-983c-49ca-b02a-e22d812f3cab",
   "metadata": {
    "id": "43901b7a-983c-49ca-b02a-e22d812f3cab"
   },
   "outputs": [],
   "source": [
    "# Autogressive vs. direct mapping\n",
    "# Batch Norm? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c020112-bb96-45d0-abf4-332956e8e544",
   "metadata": {
    "id": "1c020112-bb96-45d0-abf4-332956e8e544"
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cc5190e9-67a3-4e83-af0c-2f79f9cab867",
   "metadata": {
    "id": "cc5190e9-67a3-4e83-af0c-2f79f9cab867"
   },
   "outputs": [],
   "source": [
    "class ArgoverseDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Dataset class for Argoverse\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 data_path,\n",
    "                 sample_indices):\n",
    "        super(ArgoverseDataset, self).__init__()\n",
    "        \n",
    "        self.data_path = data_path\n",
    "        self.sample_indices = sample_indices\n",
    "        self.pkl_list = glob(os.path.join(self.data_path, '*'))\n",
    "        self.pkl_list.sort()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sample_indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        # Load one scene\n",
    "        pkl_path = self.pkl_list[self.sample_indices[idx]]\n",
    "        with open(pkl_path, 'rb') as f:\n",
    "            scene = pickle.load(f)\n",
    "            \n",
    "        # the index of agent to be predicted \n",
    "        pred_id = np.where(scene[\"track_id\"] == scene['agent_id'])[0][0]\n",
    "        \n",
    "        # input: p_in & v_in; output: p_out\n",
    "        inp_scene = np.dstack([scene['p_in'], scene['v_in']])\n",
    "        out_scene = np.dstack([scene['p_out'], scene['v_out']])\n",
    "        \n",
    "        # Normalization \n",
    "        min_vecs = np.min(inp_scene, axis = (0,1))\n",
    "        max_vecs = np.max(inp_scene, axis = (0,1))\n",
    "        \n",
    "        # Normalize by vectors\n",
    "        inp = (inp_scene[pred_id] - min_vecs)/(max_vecs - min_vecs)\n",
    "        out = (out_scene[pred_id] - min_vecs)/(max_vecs - min_vecs)\n",
    "        \n",
    "        dat = np.concatenate((inp, out), axis=0)\n",
    "        \n",
    "        train_data = []\n",
    "        window_size = 20\n",
    "        interval = 7\n",
    "        for i in range(0, len(dat), interval):\n",
    "            #print(len(dat[i:i+input_length]))\n",
    "            if i + window_size < len(dat): \n",
    "                train_data.append(dat[i:i+window_size])\n",
    "            \n",
    "        #print(len(train_data))\n",
    "        #print(train_data)\n",
    "        \n",
    "        input_seq = []\n",
    "        target_seq = []\n",
    "        for i in range(len(train_data)):\n",
    "            input_seq.append(train_data[i][:-1])\n",
    "            target_seq.append(train_data[i][1:])\n",
    "        \n",
    "        #print(input_seq)\n",
    "        \n",
    "        input_seq = np.array(input_seq, dtype=np.float32)\n",
    "        target_seq = np.array(target_seq, dtype=np.float32)\n",
    "        \n",
    "        #print(input_seq.shape)\n",
    "        #print(target_seq.shape)\n",
    "        \n",
    "        # Convert to float torch tensor\n",
    "        #return torch.from_numpy(inp).float(), torch.from_numpy(out).float() #torch.from_numpy(out[:,:2]).float()\n",
    "        return torch.from_numpy(input_seq).float(), torch.from_numpy(target_seq).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7a9237e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 30, 2)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_sample['p_out'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "834a1777",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 30, 2)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_sample['v_out'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16e5f5c2-86ff-4a6b-a585-117d0d08b8b2",
   "metadata": {
    "id": "16e5f5c2-86ff-4a6b-a585-117d0d08b8b2"
   },
   "outputs": [],
   "source": [
    "# Try different ways of normalization\n",
    "# Leverage other features. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13ae604-526c-4fff-a672-f09cd103f7c2",
   "metadata": {
    "id": "c13ae604-526c-4fff-a672-f09cd103f7c2"
   },
   "source": [
    "# Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "913d832f-4050-4c23-9d74-114595112f13",
   "metadata": {
    "id": "913d832f-4050-4c23-9d74-114595112f13"
   },
   "outputs": [],
   "source": [
    "# Grid/Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "29e5c051-43a7-4bfd-9ab0-7105693661d3",
   "metadata": {
    "id": "29e5c051-43a7-4bfd-9ab0-7105693661d3"
   },
   "outputs": [],
   "source": [
    "# batch_size = 64\n",
    "interval = 7 # sampling interval for LSTM\n",
    "window_size = 20 # number of timesteps to take as input\n",
    "batch_size = 512\n",
    "#in_dim = 19*4 # MLP\n",
    "#out_dim = 4 #30*2 # MLP\n",
    "input_size = 4 # LSTM\n",
    "output_size = 4 # LSTM (has to match input_size)\n",
    "hidden_dim = 256 #128 #32 #128\n",
    "num_layers = 1 #3\n",
    "learning_rate = 0.01\n",
    "decay_rate = 0.95\n",
    "num_epoch = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7aa146-1618-4fc2-aec1-2724a09c8bd0",
   "metadata": {
    "id": "fb7aa146-1618-4fc2-aec1-2724a09c8bd0"
   },
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d589c9c9-c46b-4b70-a515-90dd68669431",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d589c9c9-c46b-4b70-a515-90dd68669431",
    "outputId": "34773f5c-9441-4dd7-903b-970cb0e59e99"
   },
   "outputs": [],
   "source": [
    "train_path = \"../train/train\"\n",
    "\n",
    "# total number of scenes\n",
    "indices = np.arange(0, 205942)\n",
    "\n",
    "# train-valid split\n",
    "np.random.shuffle(indices)\n",
    "train_indices = indices[:180000]\n",
    "valid_indices = indices[180000:]\n",
    "\n",
    "# define datasets\n",
    "train_set = ArgoverseDataset(train_path, train_indices)\n",
    "valid_set = ArgoverseDataset(train_path, valid_indices)\n",
    "\n",
    "# create dataloaders\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_set, batch_size=batch_size, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4ab0071e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "180000"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "629f8c9d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# train_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6df2d6b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "352"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9c45bc-bdf3-4b5a-82b0-c0752c16cd2b",
   "metadata": {
    "id": "6d9c45bc-bdf3-4b5a-82b0-c0752c16cd2b"
   },
   "source": [
    "# Model, Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8c3cc020-8055-4005-ba6b-585a2e7a5fee",
   "metadata": {
    "id": "8c3cc020-8055-4005-ba6b-585a2e7a5fee"
   },
   "outputs": [],
   "source": [
    "# # RNN, LSTM, 1dCNN, Transformer\n",
    "# model = MLPNet(in_dim = in_dim, \n",
    "#                out_dim = out_dim,\n",
    "#                hidden_dim = hidden_dim, \n",
    "#                num_layers = num_layers).to(device) # move model to gpu \n",
    "\n",
    "model = MyLSTM(input_size=input_size, output_size=output_size, hidden_dim=hidden_dim, n_layers=num_layers).to(device)\n",
    "\n",
    "# Adaptive Moment Estimation computes adaptive learning rates for each parameter. \n",
    "# Compute the decaying averages of past and past squared gradients. \n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=decay_rate)  # stepwise learning rate decay\n",
    "loss_fun = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "176e40ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('baseline.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "09549dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_path = \"./train/train/\"\n",
    "# train_pkl_list = glob(os.path.join(train_path, '*'))\n",
    "# train_pkl_list.sort()\n",
    "\n",
    "# train_preds = []\n",
    "# for idx in range(3):\n",
    "#     with open(train_pkl_list[idx], 'rb') as f:\n",
    "#         train_sample = pickle.load(f)\n",
    "#         pred_id = np.where(train_sample[\"track_id\"] == train_sample['agent_id'])[0][0]\n",
    "#         inp_scene = np.dstack([train_sample['p_in'], train_sample['v_in']])\n",
    "\n",
    "#         # Normalization \n",
    "#         min_vecs = np.min(inp_scene, axis = (0,1))\n",
    "#         max_vecs = np.max(inp_scene, axis = (0,1))\n",
    "        \n",
    "#         inp = (inp_scene[pred_id] - min_vecs)/(max_vecs - min_vecs)\n",
    "        \n",
    "#         inp = torch.from_numpy(inp).float().to(device).unsqueeze(0)\n",
    "\n",
    "#         preds = model(inp).cpu().data.numpy()\n",
    "        \n",
    "#         # De-Normalization ! \n",
    "#         preds = preds * (max_vecs[:2] - min_vecs[:2]) +  min_vecs[:2]\n",
    "#         train_preds.append(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3392ee-eb57-4d8d-b6ea-49e6f38e3770",
   "metadata": {
    "id": "1c3392ee-eb57-4d8d-b6ea-49e6f38e3770"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "795977ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 5, 19, 4])\n",
      "torch.Size([512, 5, 19, 4])\n",
      "torch.Size([2560, 19, 4])\n",
      "tensor([[1.0000, 1.0000, 0.0566, 0.0473],\n",
      "        [0.9998, 0.9996, 0.0643, 0.0610],\n",
      "        [0.9995, 0.9992, 0.0543, 0.0229],\n",
      "        [0.9993, 0.9988, 0.1135, 0.1166],\n",
      "        [0.9991, 0.9984, 0.0722, 0.0608],\n",
      "        [0.9989, 0.9981, 0.1136, 0.1087],\n",
      "        [0.9987, 0.9977, 0.0845, 0.0779],\n",
      "        [0.9984, 0.9973, 0.0762, 0.0627],\n",
      "        [0.9982, 0.9969, 0.0633, 0.0508],\n",
      "        [0.9980, 0.9965, 0.0501, 0.0407],\n",
      "        [0.9978, 0.9962, 0.1454, 0.1404],\n",
      "        [0.9975, 0.9957, 0.0430, 0.0333],\n",
      "        [0.9973, 0.9954, 0.1270, 0.1274],\n",
      "        [0.9971, 0.9950, 0.0749, 0.0670],\n",
      "        [0.9969, 0.9946, 0.0619, 0.0524],\n",
      "        [0.9966, 0.9942, 0.0747, 0.0485],\n",
      "        [0.9964, 0.9938, 0.0602, 0.0563],\n",
      "        [0.9962, 0.9935, 0.1179, 0.1185],\n",
      "        [0.9959, 0.9930, 0.0306, 0.0269]])\n"
     ]
    }
   ],
   "source": [
    "for inp, tgt in train_loader:\n",
    "    print(inp.shape)\n",
    "    print(tgt.shape)\n",
    "    print(inp.view(-1, window_size-1, 4).size()) \n",
    "    print(inp[0, 0, :, :])\n",
    "    break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fa3e350d-9fca-4356-84da-7412d86667c9",
   "metadata": {
    "id": "fa3e350d-9fca-4356-84da-7412d86667c9"
   },
   "outputs": [],
   "source": [
    "def train_epoch(train_loader, model, optimizer, loss_function):\n",
    "\n",
    "    train_mse = []\n",
    "    for inp, tgt in tqdm(train_loader):\n",
    "        \n",
    "        inp = inp.view(-1, window_size-1, 4)\n",
    "        tgt = tgt.view(-1, window_size-1, 4)\n",
    "        \n",
    "        #print(inp.size())\n",
    "        \n",
    "        inp = inp.to(device)\n",
    "        tgt = tgt.to(device)\n",
    "        \n",
    "        output, hidden = model(inp)\n",
    "        output = output.to(device)\n",
    "        #print(output.shape)\n",
    "        #print(hidden.shape)\n",
    "        #print(tgt.view(-1, 4).size())\n",
    "        \n",
    "        loss = loss_function(output, tgt.view(-1, 4))\n",
    "        train_mse.append(loss.item()) \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    train_mse = round(np.sqrt(np.mean(train_mse)),5)\n",
    "    \n",
    "    return train_mse\n",
    "\n",
    "def eval_epoch(valid_loader, model, loss_function):\n",
    "    \n",
    "    valid_mse = []\n",
    "    #preds = []\n",
    "    #trues = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inp, tgt in valid_loader:\n",
    "            \n",
    "            inp = inp.view(-1, window_size-1, 4)\n",
    "            tgt = tgt.view(-1, window_size-1, 4)\n",
    "            \n",
    "            inp = inp.to(device)\n",
    "            tgt = tgt.to(device)\n",
    "            \n",
    "            loss = 0\n",
    "            output, hidden = model(inp)\n",
    "            output = output.to(device)\n",
    "                    \n",
    "            loss = loss_function(output, tgt.view(-1, 4))\n",
    "            \n",
    "            #preds.append(pred.cpu().data.numpy())\n",
    "            #trues.append(tgt.cpu().data.numpy())\n",
    "            \n",
    "            valid_mse.append(loss.item())\n",
    "            \n",
    "        #preds = np.concatenate(preds, axis = 0)  \n",
    "        #trues = np.concatenate(trues, axis = 0)  \n",
    "        valid_mse = round(np.sqrt(np.mean(valid_mse)), 5)\n",
    "    return valid_mse#, preds, trues\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f529da48-7092-4096-b95d-a3782e0a7590",
   "metadata": {
    "id": "f529da48-7092-4096-b95d-a3782e0a7590"
   },
   "outputs": [],
   "source": [
    "# Learning Rate Decay\n",
    "# Dropout\n",
    "# L1/L2 Regulization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d242f08f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "205942"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_pkl_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c15057-a83e-44b0-af97-cf28f3c98044",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 554
    },
    "id": "f5c15057-a83e-44b0-af97-cf28f3c98044",
    "outputId": "c23108e3-db6e-4d40-8242-42cf968e5c13",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]\n",
      "  0%|          | 0/352 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|          | 1/352 [00:00<04:27,  1.31it/s]\u001b[A\n",
      "  1%|          | 2/352 [00:01<04:22,  1.33it/s]\u001b[A\n",
      "  1%|          | 3/352 [00:02<04:28,  1.30it/s]\u001b[A\n",
      "  1%|          | 4/352 [00:02<04:04,  1.42it/s]\u001b[A\n",
      "  1%|▏         | 5/352 [00:03<04:15,  1.36it/s]\u001b[A\n",
      "  2%|▏         | 6/352 [00:04<04:11,  1.38it/s]\u001b[A\n",
      "  2%|▏         | 7/352 [00:05<04:03,  1.42it/s]\u001b[A\n",
      "  2%|▏         | 8/352 [00:05<04:12,  1.36it/s]\u001b[A\n",
      "  3%|▎         | 9/352 [00:06<04:06,  1.39it/s]\u001b[A\n",
      "  3%|▎         | 10/352 [00:07<04:15,  1.34it/s]\u001b[A\n",
      "  3%|▎         | 11/352 [00:08<04:08,  1.37it/s]\u001b[A\n",
      "  3%|▎         | 12/352 [00:08<04:03,  1.39it/s]\u001b[A\n",
      "  4%|▎         | 13/352 [00:09<04:00,  1.41it/s]\u001b[A\n",
      "  4%|▍         | 14/352 [00:10<04:07,  1.37it/s]\u001b[A\n",
      "  4%|▍         | 15/352 [00:10<04:10,  1.35it/s]\u001b[A\n",
      "  5%|▍         | 16/352 [00:11<04:04,  1.37it/s]\u001b[A\n",
      "  5%|▍         | 17/352 [00:12<04:02,  1.38it/s]\u001b[A\n",
      "  5%|▌         | 18/352 [00:13<03:57,  1.41it/s]\u001b[A\n",
      "  5%|▌         | 19/352 [00:13<03:51,  1.44it/s]\u001b[A\n",
      "  6%|▌         | 20/352 [00:14<03:32,  1.56it/s]\u001b[A\n",
      "  6%|▌         | 21/352 [00:14<03:39,  1.51it/s]\u001b[A\n",
      "  6%|▋         | 22/352 [00:15<03:39,  1.50it/s]\u001b[A\n",
      "  7%|▋         | 23/352 [00:16<04:09,  1.32it/s]\u001b[A\n",
      "  7%|▋         | 24/352 [00:17<03:49,  1.43it/s]\u001b[A\n",
      "  7%|▋         | 25/352 [00:17<03:44,  1.45it/s]\u001b[A\n",
      "  7%|▋         | 26/352 [00:18<03:50,  1.41it/s]\u001b[A\n",
      "  8%|▊         | 27/352 [00:19<03:57,  1.37it/s]\u001b[A\n",
      "  8%|▊         | 28/352 [00:19<03:46,  1.43it/s]\u001b[A\n",
      "  8%|▊         | 29/352 [00:20<03:34,  1.51it/s]\u001b[A\n",
      "  9%|▊         | 30/352 [00:21<03:45,  1.43it/s]\u001b[A\n",
      "  9%|▉         | 31/352 [00:21<03:29,  1.54it/s]\u001b[A\n",
      "  9%|▉         | 32/352 [00:22<03:39,  1.46it/s]\u001b[A\n",
      "  9%|▉         | 33/352 [00:23<03:44,  1.42it/s]\u001b[A\n",
      " 10%|▉         | 34/352 [00:24<03:46,  1.40it/s]\u001b[A\n",
      " 10%|▉         | 35/352 [00:24<03:52,  1.37it/s]\u001b[A\n",
      " 10%|█         | 36/352 [00:25<03:52,  1.36it/s]\u001b[A\n",
      " 11%|█         | 37/352 [00:26<03:42,  1.42it/s]\u001b[A\n",
      " 11%|█         | 38/352 [00:26<03:41,  1.42it/s]\u001b[A\n",
      " 11%|█         | 39/352 [00:27<03:44,  1.39it/s]\u001b[A\n",
      " 11%|█▏        | 40/352 [00:28<03:47,  1.37it/s]\u001b[A\n",
      " 12%|█▏        | 41/352 [00:29<03:35,  1.44it/s]\u001b[A\n",
      " 12%|█▏        | 42/352 [00:29<03:43,  1.39it/s]\u001b[A\n",
      " 12%|█▏        | 43/352 [00:30<03:42,  1.39it/s]\u001b[A\n",
      " 12%|█▎        | 44/352 [00:31<03:47,  1.35it/s]\u001b[A\n",
      " 13%|█▎        | 45/352 [00:31<03:33,  1.44it/s]\u001b[A\n",
      " 13%|█▎        | 46/352 [00:32<03:26,  1.48it/s]\u001b[A\n",
      " 13%|█▎        | 47/352 [00:33<03:29,  1.45it/s]\u001b[A\n",
      " 14%|█▎        | 48/352 [00:34<03:45,  1.35it/s]\u001b[A\n",
      " 14%|█▍        | 49/352 [00:34<03:39,  1.38it/s]\u001b[A\n",
      " 14%|█▍        | 50/352 [00:35<03:42,  1.36it/s]\u001b[A\n",
      " 14%|█▍        | 51/352 [00:36<03:45,  1.34it/s]\u001b[A\n",
      " 15%|█▍        | 52/352 [00:37<03:40,  1.36it/s]\u001b[A\n",
      " 15%|█▌        | 53/352 [00:37<03:32,  1.41it/s]\u001b[A\n",
      " 15%|█▌        | 54/352 [00:38<03:28,  1.43it/s]\u001b[A\n",
      " 16%|█▌        | 55/352 [00:39<03:20,  1.48it/s]\u001b[A\n",
      " 16%|█▌        | 56/352 [00:39<03:22,  1.46it/s]\u001b[A\n",
      " 16%|█▌        | 57/352 [00:40<03:19,  1.48it/s]\u001b[A\n",
      " 16%|█▋        | 58/352 [00:41<03:21,  1.46it/s]\u001b[A\n",
      " 17%|█▋        | 59/352 [00:41<03:12,  1.52it/s]\u001b[A\n",
      " 17%|█▋        | 60/352 [00:42<03:15,  1.50it/s]\u001b[A\n",
      " 17%|█▋        | 61/352 [00:43<03:14,  1.50it/s]\u001b[A\n",
      " 18%|█▊        | 62/352 [00:43<03:18,  1.46it/s]\u001b[A\n",
      " 18%|█▊        | 63/352 [00:44<03:15,  1.48it/s]\u001b[A\n",
      " 18%|█▊        | 64/352 [00:45<03:11,  1.51it/s]\u001b[A\n",
      " 18%|█▊        | 65/352 [00:45<03:08,  1.52it/s]\u001b[A\n",
      " 19%|█▉        | 66/352 [00:46<03:09,  1.51it/s]\u001b[A\n",
      " 19%|█▉        | 67/352 [00:47<03:02,  1.56it/s]\u001b[A\n",
      " 19%|█▉        | 68/352 [00:47<03:14,  1.46it/s]\u001b[A\n",
      " 20%|█▉        | 69/352 [00:48<03:17,  1.43it/s]\u001b[A\n",
      " 20%|█▉        | 70/352 [00:49<03:12,  1.47it/s]\u001b[A\n",
      " 20%|██        | 71/352 [00:49<03:09,  1.49it/s]\u001b[A\n",
      " 20%|██        | 72/352 [00:50<03:06,  1.50it/s]\u001b[A\n",
      " 21%|██        | 73/352 [00:51<03:02,  1.53it/s]\u001b[A\n",
      " 21%|██        | 74/352 [00:51<02:54,  1.60it/s]\u001b[A\n",
      " 21%|██▏       | 75/352 [00:52<03:01,  1.52it/s]\u001b[A\n",
      " 22%|██▏       | 76/352 [00:53<03:02,  1.51it/s]\u001b[A\n",
      " 22%|██▏       | 77/352 [00:53<03:12,  1.43it/s]\u001b[A\n",
      " 22%|██▏       | 78/352 [00:54<03:19,  1.37it/s]\u001b[A\n",
      " 22%|██▏       | 79/352 [00:55<03:12,  1.42it/s]\u001b[A\n",
      " 23%|██▎       | 80/352 [00:56<03:15,  1.39it/s]\u001b[A\n",
      " 23%|██▎       | 81/352 [00:56<03:14,  1.40it/s]\u001b[A\n",
      " 23%|██▎       | 82/352 [00:57<03:17,  1.37it/s]\u001b[A\n",
      " 24%|██▎       | 83/352 [00:58<03:21,  1.33it/s]\u001b[A\n",
      " 24%|██▍       | 84/352 [00:59<03:18,  1.35it/s]\u001b[A\n",
      " 24%|██▍       | 85/352 [00:59<03:24,  1.30it/s]\u001b[A\n",
      " 24%|██▍       | 86/352 [01:00<03:12,  1.38it/s]\u001b[A\n",
      " 25%|██▍       | 87/352 [01:01<03:09,  1.40it/s]\u001b[A\n",
      " 25%|██▌       | 88/352 [01:01<03:13,  1.36it/s]\u001b[A\n",
      " 25%|██▌       | 89/352 [01:02<03:06,  1.41it/s]\u001b[A\n",
      " 26%|██▌       | 90/352 [01:03<03:02,  1.44it/s]\u001b[A\n",
      " 26%|██▌       | 91/352 [01:04<03:04,  1.41it/s]\u001b[A\n",
      " 26%|██▌       | 92/352 [01:04<03:05,  1.41it/s]\u001b[A\n",
      " 26%|██▋       | 93/352 [01:05<03:03,  1.41it/s]\u001b[A\n",
      " 27%|██▋       | 94/352 [01:06<02:54,  1.48it/s]\u001b[A\n",
      " 27%|██▋       | 95/352 [01:06<02:54,  1.47it/s]\u001b[A\n",
      " 27%|██▋       | 96/352 [01:07<02:58,  1.43it/s]\u001b[A\n",
      " 28%|██▊       | 97/352 [01:08<02:52,  1.48it/s]\u001b[A\n",
      " 28%|██▊       | 98/352 [01:08<03:01,  1.40it/s]\u001b[A\n",
      " 28%|██▊       | 99/352 [01:09<02:54,  1.45it/s]\u001b[A\n",
      " 28%|██▊       | 100/352 [01:10<03:04,  1.37it/s]\u001b[A\n",
      " 29%|██▊       | 101/352 [01:11<03:12,  1.30it/s]\u001b[A\n",
      " 29%|██▉       | 102/352 [01:11<03:11,  1.30it/s]\u001b[A\n",
      " 29%|██▉       | 103/352 [01:12<03:10,  1.31it/s]\u001b[A\n",
      " 30%|██▉       | 104/352 [01:13<03:05,  1.34it/s]\u001b[A\n",
      " 30%|██▉       | 105/352 [01:14<03:01,  1.36it/s]\u001b[A\n",
      " 30%|███       | 106/352 [01:14<02:51,  1.43it/s]\u001b[A\n",
      " 30%|███       | 107/352 [01:15<02:58,  1.37it/s]\u001b[A\n",
      " 31%|███       | 108/352 [01:16<03:05,  1.32it/s]\u001b[A\n",
      " 31%|███       | 109/352 [01:17<03:08,  1.29it/s]\u001b[A\n",
      " 31%|███▏      | 110/352 [01:18<03:10,  1.27it/s]\u001b[A\n",
      " 32%|███▏      | 111/352 [01:18<03:24,  1.18it/s]\u001b[A\n",
      " 32%|███▏      | 112/352 [01:19<03:27,  1.16it/s]\u001b[A\n",
      " 32%|███▏      | 113/352 [01:20<03:26,  1.15it/s]\u001b[A\n",
      " 32%|███▏      | 114/352 [01:21<03:20,  1.18it/s]\u001b[A\n",
      " 33%|███▎      | 115/352 [01:22<03:18,  1.20it/s]\u001b[A\n",
      " 33%|███▎      | 116/352 [01:23<03:21,  1.17it/s]\u001b[A\n",
      " 33%|███▎      | 117/352 [01:23<03:06,  1.26it/s]\u001b[A\n",
      " 34%|███▎      | 118/352 [01:24<03:14,  1.20it/s]\u001b[A\n",
      " 34%|███▍      | 119/352 [01:25<03:15,  1.19it/s]\u001b[A\n",
      " 34%|███▍      | 120/352 [01:26<03:26,  1.13it/s]\u001b[A\n",
      " 34%|███▍      | 121/352 [01:27<03:21,  1.15it/s]\u001b[A\n",
      " 35%|███▍      | 122/352 [01:28<03:16,  1.17it/s]\u001b[A\n",
      " 35%|███▍      | 123/352 [01:29<03:08,  1.21it/s]\u001b[A\n",
      " 35%|███▌      | 124/352 [01:29<02:57,  1.29it/s]\u001b[A\n",
      " 36%|███▌      | 125/352 [01:30<03:02,  1.25it/s]\u001b[A\n",
      " 36%|███▌      | 126/352 [01:31<03:01,  1.25it/s]\u001b[A\n",
      " 36%|███▌      | 127/352 [01:32<03:04,  1.22it/s]\u001b[A\n",
      " 36%|███▋      | 128/352 [01:33<03:01,  1.23it/s]\u001b[A\n",
      " 37%|███▋      | 129/352 [01:33<03:00,  1.24it/s]\u001b[A\n",
      " 37%|███▋      | 130/352 [01:34<03:04,  1.20it/s]\u001b[A\n",
      " 37%|███▋      | 131/352 [01:35<02:57,  1.24it/s]\u001b[A\n",
      " 38%|███▊      | 132/352 [01:36<03:05,  1.18it/s]\u001b[A\n",
      " 38%|███▊      | 133/352 [01:37<03:14,  1.12it/s]\u001b[A\n",
      " 38%|███▊      | 134/352 [01:38<03:15,  1.11it/s]\u001b[A\n",
      " 38%|███▊      | 135/352 [01:39<02:59,  1.21it/s]\u001b[A\n",
      " 39%|███▊      | 136/352 [01:39<02:51,  1.26it/s]\u001b[A\n",
      " 39%|███▉      | 137/352 [01:40<02:39,  1.35it/s]\u001b[A\n",
      " 39%|███▉      | 138/352 [01:41<02:40,  1.33it/s]\u001b[A\n",
      " 39%|███▉      | 139/352 [01:41<02:37,  1.35it/s]\u001b[A\n",
      " 40%|███▉      | 140/352 [01:42<02:43,  1.29it/s]\u001b[A\n",
      " 40%|████      | 141/352 [01:43<02:37,  1.34it/s]\u001b[A\n",
      " 40%|████      | 142/352 [01:44<02:37,  1.34it/s]\u001b[A\n",
      " 41%|████      | 143/352 [01:44<02:38,  1.32it/s]\u001b[A\n",
      " 41%|████      | 144/352 [01:45<02:47,  1.24it/s]\u001b[A\n",
      " 41%|████      | 145/352 [01:46<02:42,  1.27it/s]\u001b[A\n",
      " 41%|████▏     | 146/352 [01:47<02:44,  1.25it/s]\u001b[A\n",
      " 42%|████▏     | 147/352 [01:48<02:45,  1.24it/s]\u001b[A\n",
      " 42%|████▏     | 148/352 [01:48<02:40,  1.27it/s]\u001b[A\n",
      " 42%|████▏     | 149/352 [01:49<02:50,  1.19it/s]\u001b[A\n",
      " 43%|████▎     | 150/352 [01:50<02:50,  1.18it/s]\u001b[A\n",
      " 43%|████▎     | 151/352 [01:51<02:47,  1.20it/s]\u001b[A\n",
      " 43%|████▎     | 152/352 [01:52<02:49,  1.18it/s]\u001b[A\n",
      " 43%|████▎     | 153/352 [01:53<02:50,  1.17it/s]\u001b[A\n",
      " 44%|████▍     | 154/352 [01:53<02:35,  1.28it/s]\u001b[A\n",
      " 44%|████▍     | 155/352 [01:54<02:39,  1.24it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 156/352 [01:55<02:45,  1.19it/s]\u001b[A\n",
      " 45%|████▍     | 157/352 [01:56<02:39,  1.22it/s]\u001b[A\n",
      " 45%|████▍     | 158/352 [01:57<02:31,  1.28it/s]\u001b[A\n",
      " 45%|████▌     | 159/352 [01:58<02:33,  1.25it/s]\u001b[A\n",
      " 45%|████▌     | 160/352 [01:58<02:30,  1.28it/s]\u001b[A\n",
      " 46%|████▌     | 161/352 [01:59<02:44,  1.16it/s]\u001b[A\n",
      " 46%|████▌     | 162/352 [02:00<02:55,  1.08it/s]\u001b[A\n",
      " 46%|████▋     | 163/352 [02:01<02:58,  1.06it/s]\u001b[A\n",
      " 47%|████▋     | 164/352 [02:02<02:52,  1.09it/s]\u001b[A\n",
      " 47%|████▋     | 165/352 [02:03<02:51,  1.09it/s]\u001b[A\n",
      " 47%|████▋     | 166/352 [02:04<03:02,  1.02it/s]\u001b[A\n",
      " 47%|████▋     | 167/352 [02:05<02:47,  1.10it/s]\u001b[A\n",
      " 48%|████▊     | 168/352 [02:06<02:53,  1.06it/s]\u001b[A\n",
      " 48%|████▊     | 169/352 [02:07<02:48,  1.08it/s]\u001b[A\n",
      " 48%|████▊     | 170/352 [02:08<02:41,  1.13it/s]\u001b[A\n",
      " 49%|████▊     | 171/352 [02:09<02:43,  1.11it/s]\u001b[A\n",
      " 49%|████▉     | 172/352 [02:10<02:49,  1.06it/s]\u001b[A\n",
      " 49%|████▉     | 173/352 [02:11<02:48,  1.07it/s]\u001b[A\n",
      " 49%|████▉     | 174/352 [02:12<02:44,  1.08it/s]\u001b[A\n",
      " 50%|████▉     | 175/352 [02:12<02:33,  1.16it/s]\u001b[A\n",
      " 50%|█████     | 176/352 [02:13<02:26,  1.20it/s]\u001b[A\n",
      " 50%|█████     | 177/352 [02:14<02:25,  1.20it/s]\u001b[A\n",
      " 51%|█████     | 178/352 [02:15<02:26,  1.19it/s]\u001b[A\n",
      " 51%|█████     | 179/352 [02:15<02:18,  1.25it/s]\u001b[A\n",
      " 51%|█████     | 180/352 [02:16<02:14,  1.28it/s]\u001b[A\n",
      " 51%|█████▏    | 181/352 [02:17<02:22,  1.20it/s]\u001b[A\n",
      " 52%|█████▏    | 182/352 [02:18<02:34,  1.10it/s]\u001b[A\n",
      " 52%|█████▏    | 183/352 [02:19<02:29,  1.13it/s]\u001b[A\n",
      " 52%|█████▏    | 184/352 [02:20<02:26,  1.15it/s]\u001b[A\n",
      " 53%|█████▎    | 185/352 [02:21<02:24,  1.16it/s]\u001b[A\n",
      " 53%|█████▎    | 186/352 [02:22<02:23,  1.15it/s]\u001b[A\n",
      " 53%|█████▎    | 187/352 [02:22<02:15,  1.22it/s]\u001b[A\n",
      " 53%|█████▎    | 188/352 [02:23<02:22,  1.15it/s]\u001b[A\n",
      " 54%|█████▎    | 189/352 [02:24<02:19,  1.17it/s]\u001b[A\n",
      " 54%|█████▍    | 190/352 [02:25<02:17,  1.18it/s]\u001b[A\n",
      " 54%|█████▍    | 191/352 [02:26<02:21,  1.14it/s]\u001b[A\n",
      " 55%|█████▍    | 192/352 [02:27<02:17,  1.16it/s]\u001b[A\n",
      " 55%|█████▍    | 193/352 [02:28<02:21,  1.13it/s]\u001b[A\n",
      " 55%|█████▌    | 194/352 [02:28<02:12,  1.20it/s]\u001b[A\n",
      " 55%|█████▌    | 195/352 [02:29<02:15,  1.16it/s]\u001b[A\n",
      " 56%|█████▌    | 196/352 [02:30<02:16,  1.14it/s]\u001b[A\n",
      " 56%|█████▌    | 197/352 [02:31<02:13,  1.16it/s]\u001b[A\n",
      " 56%|█████▋    | 198/352 [02:32<02:13,  1.16it/s]\u001b[A\n",
      " 57%|█████▋    | 199/352 [02:33<02:19,  1.10it/s]\u001b[A\n",
      " 57%|█████▋    | 200/352 [02:34<02:21,  1.08it/s]\u001b[A\n",
      " 57%|█████▋    | 201/352 [02:35<02:20,  1.08it/s]\u001b[A\n",
      " 57%|█████▋    | 202/352 [02:36<02:18,  1.08it/s]\u001b[A\n",
      " 58%|█████▊    | 203/352 [02:37<02:14,  1.11it/s]\u001b[A\n",
      " 58%|█████▊    | 204/352 [02:37<02:07,  1.16it/s]\u001b[A\n",
      " 58%|█████▊    | 205/352 [02:38<02:07,  1.15it/s]\u001b[A\n",
      " 59%|█████▊    | 206/352 [02:39<01:58,  1.23it/s]\u001b[A\n",
      " 59%|█████▉    | 207/352 [02:40<02:04,  1.17it/s]\u001b[A\n",
      " 59%|█████▉    | 208/352 [02:41<02:03,  1.17it/s]\u001b[A\n",
      " 59%|█████▉    | 209/352 [02:42<01:58,  1.20it/s]\u001b[A\n",
      " 60%|█████▉    | 210/352 [02:42<02:03,  1.15it/s]\u001b[A\n",
      " 60%|█████▉    | 211/352 [02:43<02:05,  1.12it/s]\u001b[A\n",
      " 60%|██████    | 212/352 [02:45<02:12,  1.06it/s]\u001b[A\n",
      " 61%|██████    | 213/352 [02:45<02:00,  1.15it/s]\u001b[A\n",
      " 61%|██████    | 214/352 [02:46<01:55,  1.19it/s]\u001b[A\n",
      " 61%|██████    | 215/352 [02:47<01:51,  1.23it/s]\u001b[A\n",
      " 61%|██████▏   | 216/352 [02:48<01:58,  1.15it/s]\u001b[A\n",
      " 62%|██████▏   | 217/352 [02:49<01:57,  1.15it/s]\u001b[A\n",
      " 62%|██████▏   | 218/352 [02:49<01:57,  1.14it/s]\u001b[A\n",
      " 62%|██████▏   | 219/352 [02:50<01:57,  1.13it/s]\u001b[A\n",
      " 62%|██████▎   | 220/352 [02:52<02:10,  1.01it/s]\u001b[A\n",
      " 63%|██████▎   | 221/352 [02:53<02:06,  1.03it/s]\u001b[A\n",
      " 63%|██████▎   | 222/352 [02:53<02:05,  1.03it/s]\u001b[A\n",
      " 63%|██████▎   | 223/352 [02:55<02:15,  1.05s/it]\u001b[A\n",
      " 64%|██████▎   | 224/352 [02:56<02:05,  1.02it/s]\u001b[A\n",
      " 64%|██████▍   | 225/352 [02:56<02:02,  1.04it/s]\u001b[A\n",
      " 64%|██████▍   | 226/352 [02:57<01:58,  1.06it/s]\u001b[A\n",
      " 64%|██████▍   | 227/352 [02:58<01:55,  1.08it/s]\u001b[A\n",
      " 65%|██████▍   | 228/352 [02:59<01:55,  1.08it/s]\u001b[A\n",
      " 65%|██████▌   | 229/352 [03:00<02:01,  1.02it/s]\u001b[A\n",
      " 65%|██████▌   | 230/352 [03:01<02:05,  1.03s/it]\u001b[A\n",
      " 66%|██████▌   | 231/352 [03:03<02:08,  1.06s/it]\u001b[A\n",
      " 66%|██████▌   | 232/352 [03:04<02:09,  1.08s/it]\u001b[A\n",
      " 66%|██████▌   | 233/352 [03:05<02:03,  1.04s/it]\u001b[A\n",
      " 66%|██████▋   | 234/352 [03:06<01:59,  1.01s/it]\u001b[A\n",
      " 67%|██████▋   | 235/352 [03:06<01:49,  1.07it/s]\u001b[A\n",
      " 67%|██████▋   | 236/352 [03:07<01:48,  1.07it/s]\u001b[A\n",
      " 67%|██████▋   | 237/352 [03:08<01:49,  1.05it/s]\u001b[A\n",
      " 68%|██████▊   | 238/352 [03:09<01:57,  1.03s/it]\u001b[A\n",
      " 68%|██████▊   | 239/352 [03:10<01:49,  1.03it/s]\u001b[A\n",
      " 68%|██████▊   | 240/352 [03:11<01:40,  1.12it/s]\u001b[A\n",
      " 68%|██████▊   | 241/352 [03:12<01:41,  1.10it/s]\u001b[A\n",
      " 69%|██████▉   | 242/352 [03:13<01:45,  1.05it/s]\u001b[A\n",
      " 69%|██████▉   | 243/352 [03:14<01:43,  1.06it/s]\u001b[A\n",
      " 69%|██████▉   | 244/352 [03:15<01:46,  1.01it/s]\u001b[A\n",
      " 70%|██████▉   | 245/352 [03:16<01:48,  1.01s/it]\u001b[A\n",
      " 70%|██████▉   | 246/352 [03:17<01:46,  1.01s/it]\u001b[A\n",
      " 70%|███████   | 247/352 [03:18<01:47,  1.02s/it]\u001b[A\n",
      " 70%|███████   | 248/352 [03:19<01:40,  1.03it/s]\u001b[A\n",
      " 71%|███████   | 249/352 [03:20<01:43,  1.01s/it]\u001b[A\n",
      " 71%|███████   | 250/352 [03:22<02:00,  1.18s/it]\u001b[A\n",
      " 71%|███████▏  | 251/352 [03:23<01:53,  1.12s/it]\u001b[A\n",
      " 72%|███████▏  | 252/352 [03:24<01:50,  1.10s/it]\u001b[A\n",
      " 72%|███████▏  | 253/352 [03:25<01:45,  1.06s/it]\u001b[A\n",
      " 72%|███████▏  | 254/352 [03:26<01:41,  1.04s/it]\u001b[A\n",
      " 72%|███████▏  | 255/352 [03:27<01:39,  1.02s/it]\u001b[A\n",
      " 73%|███████▎  | 256/352 [03:28<01:37,  1.02s/it]\u001b[A\n",
      " 73%|███████▎  | 257/352 [03:29<01:37,  1.03s/it]\u001b[A\n",
      " 73%|███████▎  | 258/352 [03:30<01:34,  1.00s/it]\u001b[A\n",
      " 74%|███████▎  | 259/352 [03:31<01:34,  1.01s/it]\u001b[A\n",
      " 74%|███████▍  | 260/352 [03:32<01:37,  1.06s/it]\u001b[A\n",
      " 74%|███████▍  | 261/352 [03:33<01:38,  1.08s/it]\u001b[A\n",
      " 74%|███████▍  | 262/352 [03:34<01:38,  1.10s/it]\u001b[A\n",
      " 75%|███████▍  | 263/352 [03:35<01:37,  1.09s/it]\u001b[A\n",
      " 75%|███████▌  | 264/352 [03:36<01:34,  1.07s/it]\u001b[A\n",
      " 75%|███████▌  | 265/352 [03:37<01:30,  1.03s/it]\u001b[A\n",
      " 76%|███████▌  | 266/352 [03:38<01:28,  1.03s/it]\u001b[A\n",
      " 76%|███████▌  | 267/352 [03:39<01:26,  1.02s/it]\u001b[A\n",
      " 76%|███████▌  | 268/352 [03:40<01:24,  1.00s/it]\u001b[A\n",
      " 76%|███████▋  | 269/352 [03:41<01:25,  1.03s/it]\u001b[A\n",
      " 77%|███████▋  | 270/352 [03:42<01:27,  1.07s/it]\u001b[A\n",
      " 77%|███████▋  | 271/352 [03:43<01:25,  1.05s/it]\u001b[A\n",
      " 77%|███████▋  | 272/352 [03:44<01:22,  1.03s/it]\u001b[A\n",
      " 78%|███████▊  | 273/352 [03:45<01:19,  1.01s/it]\u001b[A\n",
      " 78%|███████▊  | 274/352 [03:47<01:22,  1.06s/it]\u001b[A\n",
      " 78%|███████▊  | 275/352 [03:48<01:21,  1.06s/it]\u001b[A\n",
      " 78%|███████▊  | 276/352 [03:49<01:24,  1.11s/it]\u001b[A\n",
      " 79%|███████▊  | 277/352 [03:50<01:19,  1.06s/it]\u001b[A\n",
      " 79%|███████▉  | 278/352 [03:51<01:17,  1.04s/it]\u001b[A\n",
      " 79%|███████▉  | 279/352 [03:52<01:21,  1.11s/it]\u001b[A\n",
      " 80%|███████▉  | 280/352 [03:53<01:20,  1.12s/it]\u001b[A\n",
      " 80%|███████▉  | 281/352 [03:54<01:15,  1.07s/it]\u001b[A\n",
      " 80%|████████  | 282/352 [03:55<01:12,  1.04s/it]\u001b[A\n",
      " 80%|████████  | 283/352 [03:56<01:15,  1.10s/it]\u001b[A\n",
      " 81%|████████  | 284/352 [03:57<01:11,  1.06s/it]\u001b[A\n",
      " 81%|████████  | 285/352 [03:58<01:08,  1.03s/it]\u001b[A\n",
      " 81%|████████▏ | 286/352 [03:59<01:10,  1.06s/it]\u001b[A\n",
      " 82%|████████▏ | 287/352 [04:01<01:14,  1.14s/it]\u001b[A\n",
      " 82%|████████▏ | 288/352 [04:02<01:16,  1.20s/it]\u001b[A\n",
      " 82%|████████▏ | 289/352 [04:03<01:08,  1.08s/it]\u001b[A\n",
      " 82%|████████▏ | 290/352 [04:04<01:11,  1.15s/it]\u001b[A\n",
      " 83%|████████▎ | 291/352 [04:06<01:14,  1.22s/it]\u001b[A\n",
      " 83%|████████▎ | 292/352 [04:07<01:10,  1.17s/it]\u001b[A\n",
      " 83%|████████▎ | 293/352 [04:08<01:10,  1.19s/it]\u001b[A\n",
      " 84%|████████▎ | 294/352 [04:09<01:10,  1.22s/it]\u001b[A\n",
      " 84%|████████▍ | 295/352 [04:10<01:06,  1.16s/it]\u001b[A\n",
      " 84%|████████▍ | 296/352 [04:11<01:05,  1.17s/it]\u001b[A\n",
      " 84%|████████▍ | 297/352 [04:13<01:05,  1.19s/it]\u001b[A\n",
      " 85%|████████▍ | 298/352 [04:14<01:05,  1.22s/it]\u001b[A\n",
      " 85%|████████▍ | 299/352 [04:15<01:02,  1.18s/it]\u001b[A\n",
      " 85%|████████▌ | 300/352 [04:16<01:01,  1.19s/it]\u001b[A\n",
      " 86%|████████▌ | 301/352 [04:17<01:01,  1.21s/it]\u001b[A\n",
      " 86%|████████▌ | 302/352 [04:18<00:57,  1.15s/it]\u001b[A\n",
      " 86%|████████▌ | 303/352 [04:20<00:55,  1.13s/it]\u001b[A\n",
      " 86%|████████▋ | 304/352 [04:21<00:51,  1.08s/it]\u001b[A\n",
      " 87%|████████▋ | 305/352 [04:22<00:49,  1.05s/it]\u001b[A\n",
      " 87%|████████▋ | 306/352 [04:23<00:48,  1.06s/it]\u001b[A\n",
      " 87%|████████▋ | 307/352 [04:24<00:48,  1.08s/it]\u001b[A\n",
      " 88%|████████▊ | 308/352 [04:25<00:46,  1.05s/it]\u001b[A\n",
      " 88%|████████▊ | 309/352 [04:26<00:47,  1.10s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 310/352 [04:27<00:51,  1.22s/it]\u001b[A\n",
      " 88%|████████▊ | 311/352 [04:29<00:50,  1.23s/it]\u001b[A\n",
      " 89%|████████▊ | 312/352 [04:30<00:50,  1.25s/it]\u001b[A\n",
      " 89%|████████▉ | 313/352 [04:31<00:48,  1.23s/it]\u001b[A\n",
      " 89%|████████▉ | 314/352 [04:33<00:48,  1.28s/it]\u001b[A\n",
      " 89%|████████▉ | 315/352 [04:34<00:45,  1.22s/it]\u001b[A\n",
      " 90%|████████▉ | 316/352 [04:35<00:42,  1.19s/it]\u001b[A\n",
      " 90%|█████████ | 317/352 [04:36<00:41,  1.18s/it]\u001b[A\n",
      " 90%|█████████ | 318/352 [04:37<00:38,  1.12s/it]\u001b[A\n",
      " 91%|█████████ | 319/352 [04:38<00:36,  1.10s/it]\u001b[A\n",
      " 91%|█████████ | 320/352 [04:39<00:34,  1.08s/it]\u001b[A\n",
      " 91%|█████████ | 321/352 [04:40<00:33,  1.08s/it]\u001b[A\n",
      " 91%|█████████▏| 322/352 [04:41<00:35,  1.18s/it]\u001b[A\n",
      " 92%|█████████▏| 323/352 [04:43<00:39,  1.35s/it]\u001b[A\n",
      " 92%|█████████▏| 324/352 [04:44<00:36,  1.32s/it]\u001b[A\n",
      " 92%|█████████▏| 325/352 [04:46<00:34,  1.26s/it]\u001b[A\n",
      " 93%|█████████▎| 326/352 [04:47<00:32,  1.26s/it]\u001b[A\n",
      " 93%|█████████▎| 327/352 [04:48<00:30,  1.22s/it]\u001b[A\n",
      " 93%|█████████▎| 328/352 [04:49<00:30,  1.26s/it]\u001b[A\n",
      " 93%|█████████▎| 329/352 [04:51<00:28,  1.25s/it]\u001b[A\n",
      " 94%|█████████▍| 330/352 [04:52<00:27,  1.27s/it]\u001b[A\n",
      " 94%|█████████▍| 331/352 [04:53<00:25,  1.21s/it]\u001b[A\n",
      " 94%|█████████▍| 332/352 [04:54<00:22,  1.13s/it]\u001b[A\n",
      " 95%|█████████▍| 333/352 [04:55<00:20,  1.10s/it]\u001b[A\n",
      " 95%|█████████▍| 334/352 [04:56<00:20,  1.15s/it]\u001b[A\n",
      " 95%|█████████▌| 335/352 [04:57<00:19,  1.17s/it]\u001b[A\n",
      " 95%|█████████▌| 336/352 [04:59<00:19,  1.19s/it]\u001b[A\n",
      " 96%|█████████▌| 337/352 [05:00<00:17,  1.16s/it]\u001b[A\n",
      " 96%|█████████▌| 338/352 [05:01<00:16,  1.16s/it]\u001b[A\n",
      " 96%|█████████▋| 339/352 [05:02<00:14,  1.13s/it]\u001b[A\n",
      " 97%|█████████▋| 340/352 [05:03<00:13,  1.11s/it]\u001b[A\n",
      " 97%|█████████▋| 341/352 [05:05<00:14,  1.34s/it]\u001b[A\n",
      " 97%|█████████▋| 342/352 [05:06<00:13,  1.36s/it]\u001b[A\n",
      " 97%|█████████▋| 343/352 [05:08<00:11,  1.32s/it]\u001b[A\n",
      " 98%|█████████▊| 344/352 [05:09<00:10,  1.33s/it]\u001b[A\n",
      " 98%|█████████▊| 345/352 [05:10<00:09,  1.29s/it]\u001b[A\n",
      " 98%|█████████▊| 346/352 [05:11<00:07,  1.20s/it]\u001b[A\n",
      " 99%|█████████▊| 347/352 [05:12<00:06,  1.21s/it]\u001b[A\n",
      " 99%|█████████▉| 348/352 [05:13<00:04,  1.20s/it]\u001b[A\n",
      " 99%|█████████▉| 349/352 [05:15<00:03,  1.28s/it]\u001b[A\n",
      " 99%|█████████▉| 350/352 [05:16<00:02,  1.27s/it]\u001b[A\n",
      "100%|█████████▉| 351/352 [05:17<00:01,  1.24s/it]\u001b[A\n",
      "100%|██████████| 352/352 [05:18<00:00,  1.10it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.21745]\n",
      "0.11777\n",
      "Epoch 1 | T: 6.30 | Train RMSE: 0.21745 | Valid RMSE: 0.11777\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/1000 [06:18<104:58:16, 378.27s/it]\n",
      "  0%|          | 0/352 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|          | 1/352 [00:00<03:24,  1.72it/s]\u001b[A\n",
      "  1%|          | 2/352 [00:01<03:25,  1.70it/s]\u001b[A\n",
      "  1%|          | 3/352 [00:01<03:57,  1.47it/s]\u001b[A\n",
      "  1%|          | 4/352 [00:02<03:43,  1.55it/s]\u001b[A\n",
      "  1%|▏         | 5/352 [00:03<03:31,  1.64it/s]\u001b[A\n",
      "  2%|▏         | 6/352 [00:03<03:43,  1.55it/s]\u001b[A\n",
      "  2%|▏         | 7/352 [00:04<03:54,  1.47it/s]\u001b[A\n",
      "  2%|▏         | 8/352 [00:05<04:05,  1.40it/s]\u001b[A\n",
      "  3%|▎         | 9/352 [00:06<03:58,  1.44it/s]\u001b[A\n",
      "  3%|▎         | 10/352 [00:07<04:33,  1.25it/s]\u001b[A\n",
      "  3%|▎         | 11/352 [00:07<04:17,  1.33it/s]\u001b[A\n",
      "  3%|▎         | 12/352 [00:08<04:07,  1.37it/s]\u001b[A\n",
      "  4%|▎         | 13/352 [00:09<04:16,  1.32it/s]\u001b[A\n",
      "  4%|▍         | 14/352 [00:09<04:14,  1.33it/s]\u001b[A\n",
      "  4%|▍         | 15/352 [00:10<04:16,  1.32it/s]\u001b[A\n",
      "  5%|▍         | 16/352 [00:11<04:00,  1.40it/s]\u001b[A\n",
      "  5%|▍         | 17/352 [00:12<04:06,  1.36it/s]\u001b[A\n",
      "  5%|▌         | 18/352 [00:12<04:13,  1.32it/s]\u001b[A\n",
      "  5%|▌         | 19/352 [00:13<04:00,  1.38it/s]\u001b[A\n",
      "  6%|▌         | 20/352 [00:14<04:02,  1.37it/s]\u001b[A\n",
      "  6%|▌         | 21/352 [00:15<04:03,  1.36it/s]\u001b[A\n",
      "  6%|▋         | 22/352 [00:15<03:53,  1.41it/s]\u001b[A\n",
      "  7%|▋         | 23/352 [00:16<03:55,  1.40it/s]\u001b[A\n",
      "  7%|▋         | 24/352 [00:17<04:06,  1.33it/s]\u001b[A\n",
      "  7%|▋         | 25/352 [00:18<04:22,  1.25it/s]\u001b[A\n",
      "  7%|▋         | 26/352 [00:18<04:08,  1.31it/s]\u001b[A\n",
      "  8%|▊         | 27/352 [00:19<03:58,  1.36it/s]\u001b[A\n",
      "  8%|▊         | 28/352 [00:20<03:51,  1.40it/s]\u001b[A\n",
      "  8%|▊         | 29/352 [00:20<03:40,  1.47it/s]\u001b[A\n",
      "  9%|▊         | 30/352 [00:21<03:30,  1.53it/s]\u001b[A\n",
      "  9%|▉         | 31/352 [00:22<03:27,  1.55it/s]\u001b[A\n",
      "  9%|▉         | 32/352 [00:22<03:20,  1.60it/s]\u001b[A\n",
      "  9%|▉         | 33/352 [00:23<03:25,  1.55it/s]\u001b[A\n",
      " 10%|▉         | 34/352 [00:24<03:33,  1.49it/s]\u001b[A\n",
      " 10%|▉         | 35/352 [00:24<03:32,  1.49it/s]\u001b[A\n",
      " 10%|█         | 36/352 [00:25<03:28,  1.51it/s]\u001b[A\n",
      " 11%|█         | 37/352 [00:26<03:40,  1.43it/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "train_rmse = []\n",
    "valid_rmse = []\n",
    "min_rmse = 10e8\n",
    "\n",
    "for i in tqdm(range(num_epoch)):\n",
    "    start = time.time()\n",
    "\n",
    "    model.train() # if you use dropout or batchnorm. \n",
    "    train_rmse.append(train_epoch(train_loader, model, optimizer, loss_fun))\n",
    "    print(train_rmse)\n",
    "    \n",
    "    model.eval()\n",
    "    val_rmse = eval_epoch(valid_loader, model, loss_fun)\n",
    "    valid_rmse.append(val_rmse)\n",
    "    print(val_rmse)\n",
    "\n",
    "    # save the best model\n",
    "    if valid_rmse[-1] < min_rmse:\n",
    "        min_rmse = valid_rmse[-1] \n",
    "        best_model = model\n",
    "        \n",
    "        # torch.save([best_model, i, get_lr(optimizer)], name + \".pth\")\n",
    "\n",
    "    end = time.time()\n",
    "    \n",
    "    # Early Stopping\n",
    "    if (len(train_rmse) > 100 and np.mean(valid_rmse[-5:]) >= np.mean(valid_rmse[-10:-5])):\n",
    "        torch.save(best_model.state_dict(), f'lstm_hdim_{hidden_dim}_wsize_{window_size}_interval_{interval}_nlayers_{num_layers}_bs_{batch_size}_lr_{learning_rate}_decay_{decay_rate}_epoch_{i+1}.pt')    \n",
    "        break       \n",
    "\n",
    "    # Learning Rate Decay        \n",
    "    scheduler.step()\n",
    "    \n",
    "    print(\"Epoch {} | T: {:0.2f} | Train RMSE: {:0.5f} | Valid RMSE: {:0.5f}\".format(i + 1, (end-start) / 60, train_rmse[-1], valid_rmse[-1]))\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(train_rmse, label=\"train_rmse\")\n",
    "    plt.plot(valid_rmse, label=\"valid_rmse\")\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('RSME loss')\n",
    "    plt.title(f'RMSE loss curve for LSTM, hdim: {hidden_dim}, wsize: {window_size}, nlayers: {num_layers}, bs: {batch_size}, lr: {learning_rate}, decay: {decay_rate}')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'lstm_loss_curve_v1_hdim_{hidden_dim}_wsize_{window_size}_interval_{interval}_nlayers_{num_layers}_bs_{batch_size}_lr_{learning_rate}_decay_{decay_rate}.png')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540da579-31a5-462c-826b-e8e1c1a2ddd9",
   "metadata": {
    "id": "540da579-31a5-462c-826b-e8e1c1a2ddd9"
   },
   "source": [
    "# Evaluation and Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f90248e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('lstm_hdim_32_wsize_20_interval_7_nlayers_1_bs_512_lr_0.01_decay_0.95.pt'))\n",
    "best_model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cef337bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyLSTM(\n",
       "  (lstm): LSTM(4, 32, batch_first=True)\n",
       "  (fc): Linear(in_features=32, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "66654867-c45e-4ad2-bdb4-ce1d546ea2d2",
   "metadata": {
    "id": "66654867-c45e-4ad2-bdb4-ce1d546ea2d2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_path = \"../val_in/val_in/\"\n",
    "test_pkl_list = glob(os.path.join(test_path, '*'))\n",
    "test_pkl_list.sort()\n",
    "\n",
    "test_preds = []\n",
    "for idx in range(len(test_pkl_list)):\n",
    "    with open(test_pkl_list[idx], 'rb') as f:\n",
    "        test_sample = pickle.load(f)\n",
    "        pred_id = np.where(test_sample[\"track_id\"] == test_sample['agent_id'])[0][0]\n",
    "        inp_scene = np.dstack([test_sample['p_in'], test_sample['v_in']])\n",
    "\n",
    "        # Normalization \n",
    "        min_vecs = np.min(inp_scene, axis = (0,1))\n",
    "        max_vecs = np.max(inp_scene, axis = (0,1))\n",
    "        #print(min_vecs.shape)\n",
    "        #print(max_vecs.shape)\n",
    "        \n",
    "        inp = (inp_scene[pred_id] - min_vecs)/(max_vecs - min_vecs)\n",
    "        \n",
    "        inp = torch.from_numpy(inp).float().to(device).unsqueeze(0)\n",
    "\n",
    "        #print(inp)\n",
    "        # post-processing for LSTM\n",
    "        predictions = [[]]\n",
    "        inp_data = inp[0][-1]\n",
    "        #print(inp_data.size())\n",
    "        for i in range(30):\n",
    "            preds = best_model(inp_data.reshape(1, 1, 4))\n",
    "            predictions[0].append(preds[0].cpu().data.numpy()[0, :2])\n",
    "            #print(preds)\n",
    "            inp_data = preds[0]\n",
    "            \n",
    "#         print(inp[0][-1])\n",
    "#         preds = best_model(inp)#.cpu().data.numpy()\n",
    "#         print(preds)\n",
    "#         print(inp.shape)\n",
    "#         print(preds[0].shape)\n",
    "\n",
    "        predictions = np.array(predictions)\n",
    "        #print(predictions.shape)\n",
    "\n",
    "        # De-Normalization ! \n",
    "        predictions = predictions * (max_vecs[:2] - min_vecs[:2]) +  min_vecs[:2]\n",
    "        test_preds.append(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "db582d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 30, 2)\n",
      "[[[1711.24461991  339.599696  ]\n",
      "  [1708.99950078  342.39178537]\n",
      "  [1707.06558524  344.7566902 ]\n",
      "  [1705.39272282  346.76168501]\n",
      "  [1703.93939887  348.46289735]\n",
      "  [1702.67159015  349.90718381]\n",
      "  [1701.5611      351.13394056]\n",
      "  [1700.58441387  352.17628133]\n",
      "  [1699.72186694  353.06208447]\n",
      "  [1698.95733198  353.81504004]\n",
      "  [1698.27665864  354.45517335]\n",
      "  [1697.66829773  354.99939028]\n",
      "  [1697.12226079  355.46217536]\n",
      "  [1696.63001597  355.85567901]\n",
      "  [1696.18448811  356.19026292]\n",
      "  [1695.7798506   356.47480539]\n",
      "  [1695.41058895  356.71678865]\n",
      "  [1695.07222917  356.92256059]\n",
      "  [1694.76133771  357.09757471]\n",
      "  [1694.47458507  357.24641198]\n",
      "  [1694.20874582  357.37304254]\n",
      "  [1693.96205116  357.48078213]\n",
      "  [1693.73200396  357.57246659]\n",
      "  [1693.51693948  357.65047363]\n",
      "  [1693.31529703  357.7168756 ]\n",
      "  [1693.12572399  357.77346125]\n",
      "  [1692.94707585  357.82164848]\n",
      "  [1692.77841619  357.86272434]\n",
      "  [1692.61849645  357.89773589]\n",
      "  [1692.4670045   357.92764295]]]\n"
     ]
    }
   ],
   "source": [
    "print(test_preds[0].shape)\n",
    "print(test_preds[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f945f2f3-2b73-471f-91e5-87c63eb06a77",
   "metadata": {
    "id": "f945f2f3-2b73-471f-91e5-87c63eb06a77"
   },
   "source": [
    "# Generate Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "af9c27f8-a65b-48ce-861b-262c3f0422e6",
   "metadata": {
    "id": "af9c27f8-a65b-48ce-861b-262c3f0422e6"
   },
   "outputs": [],
   "source": [
    "# Submission Files\n",
    "sample_sub = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6b504543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save for later use\n",
    "predictions = np.concatenate(test_preds).reshape(len(test_preds), -1).astype(int)\n",
    "sub_df = pd.DataFrame(np.c_[sample_sub[\"ID\"], predictions], columns=[np.r_[[\"ID\"], [\"v\" + str(i) for i in range(1, 61)]]])\n",
    "sub_df.to_csv(f'test_submission_lstm_hdim_{hidden_dim}_wsize_{window_size}_interval_{interval}_nlayers_{num_layers}_bs_{batch_size}_lr_{learning_rate}_decay_{decay_rate}.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f01524a4-5473-4c1f-9991-46fd62162e20",
   "metadata": {
    "id": "f01524a4-5473-4c1f-9991-46fd62162e20"
   },
   "outputs": [],
   "source": [
    "# Convert to int\n",
    "predictions = np.concatenate(test_preds).reshape(len(test_preds), -1)\n",
    "sub_df = pd.DataFrame(np.c_[sample_sub[\"ID\"], predictions], columns=[np.r_[[\"ID\"], [\"v\" + str(i) for i in range(1, 61)]]])\n",
    "sub_df[\"ID\"] = sub_df[\"ID\"].astype(int)\n",
    "sub_df.to_csv('test_submission.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "282ca735",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>v3</th>\n",
       "      <th>v4</th>\n",
       "      <th>v5</th>\n",
       "      <th>v6</th>\n",
       "      <th>v7</th>\n",
       "      <th>v8</th>\n",
       "      <th>v9</th>\n",
       "      <th>...</th>\n",
       "      <th>v51</th>\n",
       "      <th>v52</th>\n",
       "      <th>v53</th>\n",
       "      <th>v54</th>\n",
       "      <th>v55</th>\n",
       "      <th>v56</th>\n",
       "      <th>v57</th>\n",
       "      <th>v58</th>\n",
       "      <th>v59</th>\n",
       "      <th>v60</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10002</td>\n",
       "      <td>1711.244620</td>\n",
       "      <td>339.599696</td>\n",
       "      <td>1708.999501</td>\n",
       "      <td>342.391785</td>\n",
       "      <td>1707.065585</td>\n",
       "      <td>344.756690</td>\n",
       "      <td>1705.392723</td>\n",
       "      <td>346.761685</td>\n",
       "      <td>1703.939399</td>\n",
       "      <td>...</td>\n",
       "      <td>1693.125724</td>\n",
       "      <td>357.773461</td>\n",
       "      <td>1692.947076</td>\n",
       "      <td>357.821648</td>\n",
       "      <td>1692.778416</td>\n",
       "      <td>357.862724</td>\n",
       "      <td>1692.618496</td>\n",
       "      <td>357.897736</td>\n",
       "      <td>1692.467004</td>\n",
       "      <td>357.927643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10015</td>\n",
       "      <td>724.785839</td>\n",
       "      <td>1240.929961</td>\n",
       "      <td>724.170077</td>\n",
       "      <td>1250.371301</td>\n",
       "      <td>723.666634</td>\n",
       "      <td>1258.495345</td>\n",
       "      <td>723.257290</td>\n",
       "      <td>1265.484162</td>\n",
       "      <td>722.926780</td>\n",
       "      <td>...</td>\n",
       "      <td>722.613862</td>\n",
       "      <td>1306.364275</td>\n",
       "      <td>722.690189</td>\n",
       "      <td>1306.544594</td>\n",
       "      <td>722.767501</td>\n",
       "      <td>1306.694037</td>\n",
       "      <td>722.845573</td>\n",
       "      <td>1306.816982</td>\n",
       "      <td>722.924004</td>\n",
       "      <td>1306.916849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10019</td>\n",
       "      <td>573.966905</td>\n",
       "      <td>1252.100191</td>\n",
       "      <td>574.083810</td>\n",
       "      <td>1258.208111</td>\n",
       "      <td>574.184948</td>\n",
       "      <td>1263.295085</td>\n",
       "      <td>574.281661</td>\n",
       "      <td>1267.554346</td>\n",
       "      <td>574.378696</td>\n",
       "      <td>...</td>\n",
       "      <td>576.496260</td>\n",
       "      <td>1289.966765</td>\n",
       "      <td>576.577526</td>\n",
       "      <td>1290.050063</td>\n",
       "      <td>576.656367</td>\n",
       "      <td>1290.119294</td>\n",
       "      <td>576.732782</td>\n",
       "      <td>1290.176581</td>\n",
       "      <td>576.806842</td>\n",
       "      <td>1290.223573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10028</td>\n",
       "      <td>1692.978288</td>\n",
       "      <td>314.071388</td>\n",
       "      <td>1692.665880</td>\n",
       "      <td>313.127867</td>\n",
       "      <td>1691.663756</td>\n",
       "      <td>312.309126</td>\n",
       "      <td>1690.359604</td>\n",
       "      <td>311.608876</td>\n",
       "      <td>1688.907511</td>\n",
       "      <td>...</td>\n",
       "      <td>1661.880594</td>\n",
       "      <td>308.029418</td>\n",
       "      <td>1660.980254</td>\n",
       "      <td>308.026864</td>\n",
       "      <td>1660.110954</td>\n",
       "      <td>308.026394</td>\n",
       "      <td>1659.271484</td>\n",
       "      <td>308.027671</td>\n",
       "      <td>1658.460835</td>\n",
       "      <td>308.030319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1003</td>\n",
       "      <td>2121.928830</td>\n",
       "      <td>678.789964</td>\n",
       "      <td>2119.861912</td>\n",
       "      <td>679.308870</td>\n",
       "      <td>2117.877021</td>\n",
       "      <td>679.755910</td>\n",
       "      <td>2115.971597</td>\n",
       "      <td>680.141772</td>\n",
       "      <td>2114.142825</td>\n",
       "      <td>...</td>\n",
       "      <td>2089.661137</td>\n",
       "      <td>682.681280</td>\n",
       "      <td>2089.007355</td>\n",
       "      <td>682.703405</td>\n",
       "      <td>2088.388252</td>\n",
       "      <td>682.722910</td>\n",
       "      <td>2087.802037</td>\n",
       "      <td>682.740003</td>\n",
       "      <td>2087.247685</td>\n",
       "      <td>682.754934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3195</th>\n",
       "      <td>9897</td>\n",
       "      <td>256.307767</td>\n",
       "      <td>804.455436</td>\n",
       "      <td>256.431982</td>\n",
       "      <td>803.494397</td>\n",
       "      <td>256.561905</td>\n",
       "      <td>802.691880</td>\n",
       "      <td>256.694981</td>\n",
       "      <td>802.018920</td>\n",
       "      <td>256.829658</td>\n",
       "      <td>...</td>\n",
       "      <td>259.364627</td>\n",
       "      <td>798.514890</td>\n",
       "      <td>259.460832</td>\n",
       "      <td>798.496667</td>\n",
       "      <td>259.554466</td>\n",
       "      <td>798.479610</td>\n",
       "      <td>259.645577</td>\n",
       "      <td>798.463621</td>\n",
       "      <td>259.734198</td>\n",
       "      <td>798.448411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3196</th>\n",
       "      <td>99</td>\n",
       "      <td>587.890855</td>\n",
       "      <td>1155.802464</td>\n",
       "      <td>587.922195</td>\n",
       "      <td>1156.824746</td>\n",
       "      <td>587.943984</td>\n",
       "      <td>1157.641455</td>\n",
       "      <td>587.962068</td>\n",
       "      <td>1158.299460</td>\n",
       "      <td>587.979391</td>\n",
       "      <td>...</td>\n",
       "      <td>588.506684</td>\n",
       "      <td>1160.904332</td>\n",
       "      <td>588.531705</td>\n",
       "      <td>1160.894859</td>\n",
       "      <td>588.556182</td>\n",
       "      <td>1160.884750</td>\n",
       "      <td>588.580222</td>\n",
       "      <td>1160.874359</td>\n",
       "      <td>588.603718</td>\n",
       "      <td>1160.863755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3197</th>\n",
       "      <td>9905</td>\n",
       "      <td>1755.480836</td>\n",
       "      <td>446.634373</td>\n",
       "      <td>1754.161551</td>\n",
       "      <td>448.729816</td>\n",
       "      <td>1752.537961</td>\n",
       "      <td>450.403170</td>\n",
       "      <td>1750.833643</td>\n",
       "      <td>451.769740</td>\n",
       "      <td>1749.135843</td>\n",
       "      <td>...</td>\n",
       "      <td>1725.823985</td>\n",
       "      <td>458.957692</td>\n",
       "      <td>1725.139273</td>\n",
       "      <td>459.000214</td>\n",
       "      <td>1724.479578</td>\n",
       "      <td>459.038146</td>\n",
       "      <td>1723.843743</td>\n",
       "      <td>459.072130</td>\n",
       "      <td>1723.230824</td>\n",
       "      <td>459.102699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3198</th>\n",
       "      <td>9910</td>\n",
       "      <td>576.053804</td>\n",
       "      <td>1290.611679</td>\n",
       "      <td>577.297222</td>\n",
       "      <td>1291.835656</td>\n",
       "      <td>578.458573</td>\n",
       "      <td>1292.818994</td>\n",
       "      <td>579.551766</td>\n",
       "      <td>1293.610183</td>\n",
       "      <td>580.586147</td>\n",
       "      <td>...</td>\n",
       "      <td>594.354339</td>\n",
       "      <td>1295.502707</td>\n",
       "      <td>594.763702</td>\n",
       "      <td>1295.412812</td>\n",
       "      <td>595.158185</td>\n",
       "      <td>1295.322287</td>\n",
       "      <td>595.538460</td>\n",
       "      <td>1295.231762</td>\n",
       "      <td>595.905163</td>\n",
       "      <td>1295.141394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3199</th>\n",
       "      <td>9918</td>\n",
       "      <td>588.493254</td>\n",
       "      <td>1166.825912</td>\n",
       "      <td>591.576050</td>\n",
       "      <td>1167.398704</td>\n",
       "      <td>594.299558</td>\n",
       "      <td>1167.715848</td>\n",
       "      <td>596.795735</td>\n",
       "      <td>1167.877385</td>\n",
       "      <td>599.126729</td>\n",
       "      <td>...</td>\n",
       "      <td>631.285907</td>\n",
       "      <td>1164.532412</td>\n",
       "      <td>632.323011</td>\n",
       "      <td>1164.365439</td>\n",
       "      <td>633.328323</td>\n",
       "      <td>1164.202420</td>\n",
       "      <td>634.302991</td>\n",
       "      <td>1164.043354</td>\n",
       "      <td>635.247958</td>\n",
       "      <td>1163.888453</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3200 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID           v1           v2           v3           v4           v5  \\\n",
       "0     10002  1711.244620   339.599696  1708.999501   342.391785  1707.065585   \n",
       "1     10015   724.785839  1240.929961   724.170077  1250.371301   723.666634   \n",
       "2     10019   573.966905  1252.100191   574.083810  1258.208111   574.184948   \n",
       "3     10028  1692.978288   314.071388  1692.665880   313.127867  1691.663756   \n",
       "4      1003  2121.928830   678.789964  2119.861912   679.308870  2117.877021   \n",
       "...     ...          ...          ...          ...          ...          ...   \n",
       "3195   9897   256.307767   804.455436   256.431982   803.494397   256.561905   \n",
       "3196     99   587.890855  1155.802464   587.922195  1156.824746   587.943984   \n",
       "3197   9905  1755.480836   446.634373  1754.161551   448.729816  1752.537961   \n",
       "3198   9910   576.053804  1290.611679   577.297222  1291.835656   578.458573   \n",
       "3199   9918   588.493254  1166.825912   591.576050  1167.398704   594.299558   \n",
       "\n",
       "               v6           v7           v8           v9  ...          v51  \\\n",
       "0      344.756690  1705.392723   346.761685  1703.939399  ...  1693.125724   \n",
       "1     1258.495345   723.257290  1265.484162   722.926780  ...   722.613862   \n",
       "2     1263.295085   574.281661  1267.554346   574.378696  ...   576.496260   \n",
       "3      312.309126  1690.359604   311.608876  1688.907511  ...  1661.880594   \n",
       "4      679.755910  2115.971597   680.141772  2114.142825  ...  2089.661137   \n",
       "...           ...          ...          ...          ...  ...          ...   \n",
       "3195   802.691880   256.694981   802.018920   256.829658  ...   259.364627   \n",
       "3196  1157.641455   587.962068  1158.299460   587.979391  ...   588.506684   \n",
       "3197   450.403170  1750.833643   451.769740  1749.135843  ...  1725.823985   \n",
       "3198  1292.818994   579.551766  1293.610183   580.586147  ...   594.354339   \n",
       "3199  1167.715848   596.795735  1167.877385   599.126729  ...   631.285907   \n",
       "\n",
       "              v52          v53          v54          v55          v56  \\\n",
       "0      357.773461  1692.947076   357.821648  1692.778416   357.862724   \n",
       "1     1306.364275   722.690189  1306.544594   722.767501  1306.694037   \n",
       "2     1289.966765   576.577526  1290.050063   576.656367  1290.119294   \n",
       "3      308.029418  1660.980254   308.026864  1660.110954   308.026394   \n",
       "4      682.681280  2089.007355   682.703405  2088.388252   682.722910   \n",
       "...           ...          ...          ...          ...          ...   \n",
       "3195   798.514890   259.460832   798.496667   259.554466   798.479610   \n",
       "3196  1160.904332   588.531705  1160.894859   588.556182  1160.884750   \n",
       "3197   458.957692  1725.139273   459.000214  1724.479578   459.038146   \n",
       "3198  1295.502707   594.763702  1295.412812   595.158185  1295.322287   \n",
       "3199  1164.532412   632.323011  1164.365439   633.328323  1164.202420   \n",
       "\n",
       "              v57          v58          v59          v60  \n",
       "0     1692.618496   357.897736  1692.467004   357.927643  \n",
       "1      722.845573  1306.816982   722.924004  1306.916849  \n",
       "2      576.732782  1290.176581   576.806842  1290.223573  \n",
       "3     1659.271484   308.027671  1658.460835   308.030319  \n",
       "4     2087.802037   682.740003  2087.247685   682.754934  \n",
       "...           ...          ...          ...          ...  \n",
       "3195   259.645577   798.463621   259.734198   798.448411  \n",
       "3196   588.580222  1160.874359   588.603718  1160.863755  \n",
       "3197  1723.843743   459.072130  1723.230824   459.102699  \n",
       "3198   595.538460  1295.231762   595.905163  1295.141394  \n",
       "3199   634.302991  1164.043354   635.247958  1163.888453  \n",
       "\n",
       "[3200 rows x 61 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db08b55e-05e6-409e-b831-5f23ccd42ab3",
   "metadata": {
    "id": "db08b55e-05e6-409e-b831-5f23ccd42ab3"
   },
   "outputs": [],
   "source": [
    "# Ensemble Method "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
