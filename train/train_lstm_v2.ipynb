{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea9f615f-1564-401f-9a74-8ac98f9bbeab",
   "metadata": {
    "id": "ea9f615f-1564-401f-9a74-8ac98f9bbeab",
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "import pickle\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8845100d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4f04ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"../train/train\"\n",
    "# The glob module finds all the pathnames matching a specified pattern\n",
    "train_pkl_lst = glob(os.path.join(train_path, '*'))\n",
    "#with open(train_pkl_lst[1], 'rb') as f:\n",
    "#    training_sample = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44b0f51-0779-49dc-bcab-5c284039d6fd",
   "metadata": {
    "id": "f44b0f51-0779-49dc-bcab-5c284039d6fd"
   },
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "68fac99f-9f5e-4fb5-ba6c-e22cc6f8f8be",
   "metadata": {
    "id": "68fac99f-9f5e-4fb5-ba6c-e22cc6f8f8be"
   },
   "outputs": [],
   "source": [
    "class MyLSTM(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, n_layers):\n",
    "        super(MyLSTM, self).__init__()\n",
    "\n",
    "        # Defining some parameters\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        #Defining the layers\n",
    "        # RNN Layer\n",
    "    \n",
    "        self.lstm = nn.LSTM(input_size, hidden_dim, n_layers, batch_first=True)   \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        #Initializing hidden state for first input using method defined below\n",
    "        h_t, c_t = self.init_hidden(batch_size)\n",
    "\n",
    "        #print(x.size())\n",
    "        #print(x.size(-1))\n",
    "        # Passing in the input and hidden state into the model and obtaining outputs\n",
    "        #print(x.shape)\n",
    "        #print(h_t.shape)\n",
    "        #print(c_t.shape)\n",
    "\n",
    "        #print(f'forward pass: input shape is {x.shape}')\n",
    "        out, (h_t, c_t) = self.lstm(x, (h_t, c_t))\n",
    "        #print(f'forward pass: lstm output is {out.shape}')\n",
    "        \n",
    "        # Reshaping the outputs such that it can be fit into the fully connected layer\n",
    "        out = out.contiguous().view(-1, self.hidden_dim)\n",
    "        #print(f'forward pass: result of reshaping output before passing to fc layer is {out.shape}')\n",
    "        out = self.fc(out)\n",
    "        #print(f'forward pass: fc output is {out.shape}')\n",
    "        \n",
    "        return out, h_t\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        # This method generates the first hidden state of zeros which we'll use in the forward pass\n",
    "        h_0 = torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(device)\n",
    "        c_0 =  torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(device)       \n",
    "         # We'll send the tensor holding the hidden state to the device we specified earlier as well\n",
    "        return h_0, c_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43901b7a-983c-49ca-b02a-e22d812f3cab",
   "metadata": {
    "id": "43901b7a-983c-49ca-b02a-e22d812f3cab"
   },
   "outputs": [],
   "source": [
    "# Autogressive vs. direct mapping\n",
    "# Batch Norm? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c020112-bb96-45d0-abf4-332956e8e544",
   "metadata": {
    "id": "1c020112-bb96-45d0-abf4-332956e8e544"
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "cc5190e9-67a3-4e83-af0c-2f79f9cab867",
   "metadata": {
    "id": "cc5190e9-67a3-4e83-af0c-2f79f9cab867"
   },
   "outputs": [],
   "source": [
    "class ArgoverseDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Dataset class for Argoverse\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 data_path,\n",
    "                 sample_indices):\n",
    "        super(ArgoverseDataset, self).__init__()\n",
    "        \n",
    "        self.data_path = data_path\n",
    "        self.sample_indices = sample_indices\n",
    "        self.pkl_list = glob(os.path.join(self.data_path, '*'))\n",
    "        self.pkl_list.sort()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sample_indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        # Load one scene\n",
    "        pkl_path = self.pkl_list[self.sample_indices[idx]]\n",
    "        with open(pkl_path, 'rb') as f:\n",
    "            scene = pickle.load(f)\n",
    "            \n",
    "        # the index of agent to be predicted \n",
    "        pred_id = np.where(scene[\"track_id\"] == scene['agent_id'])[0][0]\n",
    "        \n",
    "        # input: p_in & v_in; output: p_out\n",
    "        inp_scene = np.dstack([scene['p_in'], scene['v_in']])\n",
    "        out_scene = np.dstack([scene['p_out'], scene['v_out']])\n",
    "        \n",
    "        # Normalization \n",
    "        min_vecs = np.min(inp_scene, axis = (0,1))\n",
    "        max_vecs = np.max(inp_scene, axis = (0,1))\n",
    "        \n",
    "        # Normalize by vectors\n",
    "        inp = (inp_scene[pred_id] - min_vecs)/(max_vecs - min_vecs)\n",
    "        out = (out_scene[pred_id] - min_vecs)/(max_vecs - min_vecs)\n",
    "        \n",
    "        #print(inp.shape)\n",
    "        #print(out.shape)\n",
    "        #print(inp.flatten().shape)\n",
    "        #inp = inp.reshape(1, -1)\n",
    "        out = out[:, :2].reshape(-1) # reshape to 60x1 so it matches shape of model output\n",
    "        \n",
    "        \n",
    "        \n",
    "        #inp = inp.flatten()\n",
    "        #out = out[:, :2].flatten()\n",
    "        \n",
    "        #inp = inp.reshape(-1, 1)\n",
    "        #out = out[:, :2].reshape(-1, 1)\n",
    "        \n",
    "        return torch.from_numpy(inp).float(), torch.from_numpy(out).float()\n",
    "    \n",
    "        #dat = np.concatenate((inp, out), axis=0)\n",
    "        \n",
    "        #train_data = []\n",
    "        #window_size = 20\n",
    "        #interval = 7\n",
    "        #for i in range(0, len(dat), interval):\n",
    "        #    #print(len(dat[i:i+input_length]))\n",
    "        #    if i + window_size < len(dat): \n",
    "        #        train_data.append(dat[i:i+window_size])\n",
    "            \n",
    "        #print(len(train_data))\n",
    "        #print(train_data)\n",
    "        \n",
    "        #input_seq = []\n",
    "        #target_seq = []\n",
    "        #for i in range(len(train_data)):\n",
    "        #    input_seq.append(train_data[i][:-1])\n",
    "        #    target_seq.append(train_data[i][1:])\n",
    "        \n",
    "        #print(input_seq)\n",
    "        \n",
    "        #input_seq = np.array(input_seq, dtype=np.float32)\n",
    "        #target_seq = np.array(target_seq, dtype=np.float32)\n",
    "        \n",
    "        #print(input_seq.shape)\n",
    "        #print(target_seq.shape)\n",
    "        \n",
    "        ## Convert to float torch tensor\n",
    "        ##return torch.from_numpy(inp).float(), torch.from_numpy(out).float() #torch.from_numpy(out[:,:2]).float()\n",
    "        #return torch.from_numpy(input_seq).float(), torch.from_numpy(target_seq).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16e5f5c2-86ff-4a6b-a585-117d0d08b8b2",
   "metadata": {
    "id": "16e5f5c2-86ff-4a6b-a585-117d0d08b8b2"
   },
   "outputs": [],
   "source": [
    "# Try different ways of normalization\n",
    "# Leverage other features. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13ae604-526c-4fff-a672-f09cd103f7c2",
   "metadata": {
    "id": "c13ae604-526c-4fff-a672-f09cd103f7c2"
   },
   "source": [
    "# Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "913d832f-4050-4c23-9d74-114595112f13",
   "metadata": {
    "id": "913d832f-4050-4c23-9d74-114595112f13"
   },
   "outputs": [],
   "source": [
    "# Grid/Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "29e5c051-43a7-4bfd-9ab0-7105693661d3",
   "metadata": {
    "id": "29e5c051-43a7-4bfd-9ab0-7105693661d3"
   },
   "outputs": [],
   "source": [
    "# batch_size = 64\n",
    "interval = 7 # sampling interval for LSTM\n",
    "window_size = 20 # number of timesteps to take as input\n",
    "batch_size = 512\n",
    "#in_dim = 19*4 # MLP\n",
    "#out_dim = 4 #30*2 # MLP\n",
    "input_size = 4 #19*4 #1#19*4 # LSTM\n",
    "output_size = 30*2 # LSTM (has to match input_size)\n",
    "hidden_dim = 32 #128 #256 #128 #32 #128\n",
    "num_layers = 1 #1 #3\n",
    "learning_rate = 0.01\n",
    "decay_rate = 0.95\n",
    "num_epoch = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7aa146-1618-4fc2-aec1-2724a09c8bd0",
   "metadata": {
    "id": "fb7aa146-1618-4fc2-aec1-2724a09c8bd0"
   },
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "d589c9c9-c46b-4b70-a515-90dd68669431",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d589c9c9-c46b-4b70-a515-90dd68669431",
    "outputId": "34773f5c-9441-4dd7-903b-970cb0e59e99"
   },
   "outputs": [],
   "source": [
    "train_path = \"../train/train\"\n",
    "\n",
    "# total number of scenes\n",
    "indices = np.arange(0, 205942)\n",
    "\n",
    "# train-valid split\n",
    "np.random.shuffle(indices)\n",
    "train_indices = indices[:180000]\n",
    "valid_indices = indices[180000:]\n",
    "\n",
    "# define datasets\n",
    "train_set = ArgoverseDataset(train_path, train_indices)\n",
    "valid_set = ArgoverseDataset(train_path, valid_indices)\n",
    "\n",
    "# create dataloaders\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_set, batch_size=batch_size, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "4ab0071e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "180000"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "629f8c9d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# train_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "6df2d6b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "352"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9c45bc-bdf3-4b5a-82b0-c0752c16cd2b",
   "metadata": {
    "id": "6d9c45bc-bdf3-4b5a-82b0-c0752c16cd2b"
   },
   "source": [
    "# Model, Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "8c3cc020-8055-4005-ba6b-585a2e7a5fee",
   "metadata": {
    "id": "8c3cc020-8055-4005-ba6b-585a2e7a5fee"
   },
   "outputs": [],
   "source": [
    "# # RNN, LSTM, 1dCNN, Transformer\n",
    "# model = MLPNet(in_dim = in_dim, \n",
    "#                out_dim = out_dim,\n",
    "#                hidden_dim = hidden_dim, \n",
    "#                num_layers = num_layers).to(device) # move model to gpu \n",
    "\n",
    "model = MyLSTM(input_size=input_size, output_size=output_size, hidden_dim=hidden_dim, n_layers=num_layers).to(device)\n",
    "\n",
    "# Adaptive Moment Estimation computes adaptive learning rates for each parameter. \n",
    "# Compute the decaying averages of past and past squared gradients. \n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=decay_rate)  # stepwise learning rate decay\n",
    "loss_fun = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3392ee-eb57-4d8d-b6ea-49e6f38e3770",
   "metadata": {
    "id": "1c3392ee-eb57-4d8d-b6ea-49e6f38e3770"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "795977ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "(19, 4)\n",
      "(30, 4)\n",
      "torch.Size([512, 19, 4])\n",
      "torch.Size([512, 30, 4])\n"
     ]
    }
   ],
   "source": [
    "for inp, tgt in train_loader:\n",
    "    print(inp.shape)\n",
    "    print(tgt.shape)\n",
    "    break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "fa3e350d-9fca-4356-84da-7412d86667c9",
   "metadata": {
    "id": "fa3e350d-9fca-4356-84da-7412d86667c9"
   },
   "outputs": [],
   "source": [
    "def train_epoch(train_loader, model, optimizer, loss_function):\n",
    "\n",
    "    train_mse = []\n",
    "    for inp, tgt in tqdm(train_loader):\n",
    "        \n",
    "        #inp = inp.view(-1, window_size-1, 4)\n",
    "        #tgt = tgt.view(-1, window_size-1, 4)\n",
    "        \n",
    "        #print(inp.size())\n",
    "        \n",
    "        #print(inp.shape)\n",
    "        #print(tgt.shape)\n",
    "        \n",
    "        inp = inp.to(device)\n",
    "        tgt = tgt.to(device)\n",
    "        \n",
    "        output, hidden = model(inp)\n",
    "        output = output.to(device)\n",
    "        \n",
    "        print(output.shape)\n",
    "        print(hidden.shape)\n",
    "        print(tgt.shape)\n",
    "        #print(output.shape)\n",
    "        #print(hidden.shape)\n",
    "        #print(tgt.view(-1, 4).size())\n",
    "        \n",
    "        \n",
    "        \n",
    "        loss = loss_function(output, tgt)\n",
    "        train_mse.append(loss.item()) \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    train_mse = round(np.sqrt(np.mean(train_mse)),5)\n",
    "    \n",
    "    return train_mse\n",
    "\n",
    "def eval_epoch(valid_loader, model, loss_function):\n",
    "    \n",
    "    valid_mse = []\n",
    "    #preds = []\n",
    "    #trues = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inp, tgt in valid_loader:\n",
    "            \n",
    "            #inp = inp.view(-1, window_size-1, 4)\n",
    "            #tgt = tgt.view(-1, window_size-1, 4)\n",
    "            \n",
    "            inp = inp.to(device)\n",
    "            tgt = tgt.to(device)\n",
    "            \n",
    "            loss = 0\n",
    "            output, hidden = model(inp)\n",
    "            output = output.to(device)\n",
    "                \n",
    "            loss = loss_function(output, tgt)\n",
    "            \n",
    "            #preds.append(pred.cpu().data.numpy())\n",
    "            #trues.append(tgt.cpu().data.numpy())\n",
    "            \n",
    "            valid_mse.append(loss.item())\n",
    "            \n",
    "        #preds = np.concatenate(preds, axis = 0)  \n",
    "        #trues = np.concatenate(trues, axis = 0)  \n",
    "        valid_mse = round(np.sqrt(np.mean(valid_mse)), 5)\n",
    "    return valid_mse#, preds, trues\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f529da48-7092-4096-b95d-a3782e0a7590",
   "metadata": {
    "id": "f529da48-7092-4096-b95d-a3782e0a7590"
   },
   "outputs": [],
   "source": [
    "# Learning Rate Decay\n",
    "# Dropout\n",
    "# L1/L2 Regulization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "3f7797c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "205942"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_pkl_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "f5c15057-a83e-44b0-af97-cf28f3c98044",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 554
    },
    "id": "f5c15057-a83e-44b0-af97-cf28f3c98044",
    "outputId": "c23108e3-db6e-4d40-8242-42cf968e5c13",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]\n",
      "  0%|          | 0/352 [00:00<?, ?it/s]\u001b[A/opt/conda/lib/python3.9/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([512, 60])) that is different to the input size (torch.Size([9728, 60])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "  0%|          | 0/352 [00:00<?, ?it/s]\n",
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9728, 60])\n",
      "torch.Size([1, 512, 32])\n",
      "torch.Size([512, 60])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (9728) must match the size of tensor b (512) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_18281/2353138401.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# if you use dropout or batchnorm.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mtrain_rmse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fun\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_rmse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_18281/4208054745.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(train_loader, model, optimizer, loss_function)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mtrain_mse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3087\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3088\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3089\u001b[0;31m     \u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3090\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3091\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/functional.py\u001b[0m in \u001b[0;36mbroadcast_tensors\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (9728) must match the size of tensor b (512) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "train_rmse = []\n",
    "valid_rmse = []\n",
    "min_rmse = 10e8\n",
    "\n",
    "for i in tqdm(range(num_epoch)):\n",
    "    start = time.time()\n",
    "\n",
    "    model.train() # if you use dropout or batchnorm. \n",
    "    train_rmse.append(train_epoch(train_loader, model, optimizer, loss_fun))\n",
    "    print(train_rmse)\n",
    "    \n",
    "    model.eval()\n",
    "    val_rmse = eval_epoch(valid_loader, model, loss_fun)\n",
    "    valid_rmse.append(val_rmse)\n",
    "    print(val_rmse)\n",
    "\n",
    "    # save the best model\n",
    "    if valid_rmse[-1] < min_rmse:\n",
    "        min_rmse = valid_rmse[-1] \n",
    "        best_model = model\n",
    "        \n",
    "        # torch.save([best_model, i, get_lr(optimizer)], name + \".pth\")\n",
    "\n",
    "    end = time.time()\n",
    "    \n",
    "    # Early Stopping\n",
    "    if (len(train_rmse) > 100 and np.mean(valid_rmse[-5:]) >= np.mean(valid_rmse[-10:-5])):\n",
    "        torch.save(best_model.state_dict(), f'lstm_hdim_{hidden_dim}_wsize_{window_size}_interval_{interval}_nlayers_{num_layers}_bs_{batch_size}_lr_{learning_rate}_decay_{decay_rate}_epoch_{i+1}.pt')    \n",
    "        break       \n",
    "\n",
    "    # Learning Rate Decay        \n",
    "    scheduler.step()\n",
    "    \n",
    "    print(\"Epoch {} | T: {:0.2f} | Train RMSE: {:0.5f} | Valid RMSE: {:0.5f}\".format(i + 1, (end-start) / 60, train_rmse[-1], valid_rmse[-1]))\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(train_rmse, label=\"train_rmse\")\n",
    "    plt.plot(valid_rmse, label=\"valid_rmse\")\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('RSME loss')\n",
    "    plt.title(f'RMSE loss curve for LSTM, hdim: {hidden_dim}, wsize: {window_size}, nlayers: {num_layers}, bs: {batch_size}, lr: {learning_rate}, decay: {decay_rate}')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'lstm_loss_curve_v1_hdim_{hidden_dim}_wsize_{window_size}_interval_{interval}_nlayers_{num_layers}_bs_{batch_size}_lr_{learning_rate}_decay_{decay_rate}.png')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540da579-31a5-462c-826b-e8e1c1a2ddd9",
   "metadata": {
    "id": "540da579-31a5-462c-826b-e8e1c1a2ddd9"
   },
   "source": [
    "# Evaluation and Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f90248e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('lstm_hdim_32_wsize_20_interval_7_nlayers_1_bs_512_lr_0.01_decay_0.95.pt'))\n",
    "best_model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "cef337bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyLSTM(\n",
       "  (lstm): LSTM(4, 128, num_layers=2, batch_first=True)\n",
       "  (fc): Linear(in_features=128, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "66654867-c45e-4ad2-bdb4-ce1d546ea2d2",
   "metadata": {
    "id": "66654867-c45e-4ad2-bdb4-ce1d546ea2d2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_path = \"../val_in/val_in\"\n",
    "test_pkl_list = glob(os.path.join(test_path, '*'))\n",
    "test_pkl_list.sort()\n",
    "\n",
    "test_preds = []\n",
    "for idx in range(len(test_pkl_list)):\n",
    "    with open(test_pkl_list[idx], 'rb') as f:\n",
    "        test_sample = pickle.load(f)\n",
    "        pred_id = np.where(test_sample[\"track_id\"] == test_sample['agent_id'])[0][0]\n",
    "        inp_scene = np.dstack([test_sample['p_in'], test_sample['v_in']])\n",
    "\n",
    "        # Normalization \n",
    "        min_vecs = np.min(inp_scene, axis = (0,1))\n",
    "        max_vecs = np.max(inp_scene, axis = (0,1))\n",
    "        \n",
    "        inp = (inp_scene[pred_id] - min_vecs)/(max_vecs - min_vecs)\n",
    "        \n",
    "        inp = inp.reshape(1, -1)\n",
    "        \n",
    "        inp_data = torch.from_numpy(inp).float().unsqueeze(0).to(device)\n",
    "\n",
    "        # post-processing for LSTM\n",
    "        \n",
    "        \n",
    "        output, hidden = best_model(inp_data)\n",
    "        predictions = output.cpu().data.numpy().reshape(30, 2)\n",
    "\n",
    "        # De-Normalization ! \n",
    "        predictions = predictions * (max_vecs[:2] - min_vecs[:2]) +  min_vecs[:2]\n",
    "        test_preds.append(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "db582d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 2)\n",
      "[[1735.15464966  338.65479859]\n",
      " [1733.94562731  338.94213326]\n",
      " [1735.31051158  339.49985309]\n",
      " [1734.65012734  340.10556383]\n",
      " [1736.69297974  340.0074006 ]\n",
      " [1736.1030351   340.40336924]\n",
      " [1735.38636161  341.4348248 ]\n",
      " [1735.06464929  341.70955095]\n",
      " [1737.87630256  342.00207736]\n",
      " [1736.97182483  342.60859522]\n",
      " [1735.18711223  342.99300241]\n",
      " [1735.82314954  343.13570993]\n",
      " [1737.36782647  343.9364819 ]\n",
      " [1737.29353714  344.03207107]\n",
      " [1738.58891824  344.45796859]\n",
      " [1737.26482026  345.42945732]\n",
      " [1738.84851469  345.50824967]\n",
      " [1739.73207904  346.4753756 ]\n",
      " [1741.91726729  346.63976628]\n",
      " [1740.41504141  346.9293478 ]\n",
      " [1741.84245773  347.44664619]\n",
      " [1741.93256215  347.72150322]\n",
      " [1743.29234814  347.58586346]\n",
      " [1742.43354689  348.83925516]\n",
      " [1737.83187463  349.11330508]\n",
      " [1741.5369767   349.36074185]\n",
      " [1743.82818851  349.77896082]\n",
      " [1740.94463898  350.42629277]\n",
      " [1744.55370597  350.96289659]\n",
      " [1741.52386682  350.95135696]]\n"
     ]
    }
   ],
   "source": [
    "print(test_preds[0].shape)\n",
    "print(test_preds[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f945f2f3-2b73-471f-91e5-87c63eb06a77",
   "metadata": {
    "id": "f945f2f3-2b73-471f-91e5-87c63eb06a77"
   },
   "source": [
    "# Generate Submission File"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec024e36",
   "metadata": {},
   "source": [
    "### Steps to create submission file \n",
    "Run the below cells. The last cell will generate a submission file \"test_submission.csv\" that you can submit to Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "af9c27f8-a65b-48ce-861b-262c3f0422e6",
   "metadata": {
    "id": "af9c27f8-a65b-48ce-861b-262c3f0422e6"
   },
   "outputs": [],
   "source": [
    "# Submission Files\n",
    "sample_sub = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "6b504543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save for later use\n",
    "predictions = np.concatenate(test_preds).reshape(len(test_preds), -1)#.astype(int)\n",
    "sub_df = pd.DataFrame(np.c_[sample_sub[\"ID\"], predictions], columns=[np.r_[[\"ID\"], [\"v\" + str(i) for i in range(1, 61)]]])\n",
    "sub_df[\"ID\"] = sub_df[\"ID\"].astype(int)\n",
    "sub_df.to_csv(f'test_submission_lstm_hdim_{hidden_dim}_wsize_{window_size}_interval_{interval}_nlayers_{num_layers}_bs_{batch_size}_lr_{learning_rate}_decay_{decay_rate}.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "f01524a4-5473-4c1f-9991-46fd62162e20",
   "metadata": {
    "id": "f01524a4-5473-4c1f-9991-46fd62162e20"
   },
   "outputs": [],
   "source": [
    "# Convert to float\n",
    "predictions = np.concatenate(test_preds).reshape(len(test_preds), -1)\n",
    "sub_df = pd.DataFrame(np.c_[sample_sub[\"ID\"], predictions], columns=[np.r_[[\"ID\"], [\"v\" + str(i) for i in range(1, 61)]]])\n",
    "sub_df[\"ID\"] = sub_df[\"ID\"].astype(int)\n",
    "sub_df.to_csv('test_submission.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "282ca735",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>v3</th>\n",
       "      <th>v4</th>\n",
       "      <th>v5</th>\n",
       "      <th>v6</th>\n",
       "      <th>v7</th>\n",
       "      <th>v8</th>\n",
       "      <th>v9</th>\n",
       "      <th>...</th>\n",
       "      <th>v51</th>\n",
       "      <th>v52</th>\n",
       "      <th>v53</th>\n",
       "      <th>v54</th>\n",
       "      <th>v55</th>\n",
       "      <th>v56</th>\n",
       "      <th>v57</th>\n",
       "      <th>v58</th>\n",
       "      <th>v59</th>\n",
       "      <th>v60</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10002</td>\n",
       "      <td>1735.154650</td>\n",
       "      <td>338.654799</td>\n",
       "      <td>1733.945627</td>\n",
       "      <td>338.942133</td>\n",
       "      <td>1735.310512</td>\n",
       "      <td>339.499853</td>\n",
       "      <td>1734.650127</td>\n",
       "      <td>340.105564</td>\n",
       "      <td>1736.692980</td>\n",
       "      <td>...</td>\n",
       "      <td>1741.536977</td>\n",
       "      <td>349.360742</td>\n",
       "      <td>1743.828189</td>\n",
       "      <td>349.778961</td>\n",
       "      <td>1740.944639</td>\n",
       "      <td>350.426293</td>\n",
       "      <td>1744.553706</td>\n",
       "      <td>350.962897</td>\n",
       "      <td>1741.523867</td>\n",
       "      <td>350.951357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10015</td>\n",
       "      <td>726.124713</td>\n",
       "      <td>1230.547001</td>\n",
       "      <td>725.077134</td>\n",
       "      <td>1230.135992</td>\n",
       "      <td>725.860099</td>\n",
       "      <td>1229.237264</td>\n",
       "      <td>725.632283</td>\n",
       "      <td>1229.980740</td>\n",
       "      <td>725.662231</td>\n",
       "      <td>...</td>\n",
       "      <td>725.162504</td>\n",
       "      <td>1224.285745</td>\n",
       "      <td>724.304063</td>\n",
       "      <td>1224.416170</td>\n",
       "      <td>724.419292</td>\n",
       "      <td>1223.590094</td>\n",
       "      <td>724.567961</td>\n",
       "      <td>1224.685135</td>\n",
       "      <td>724.055162</td>\n",
       "      <td>1224.853199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10019</td>\n",
       "      <td>572.624761</td>\n",
       "      <td>1245.055487</td>\n",
       "      <td>573.026634</td>\n",
       "      <td>1244.970067</td>\n",
       "      <td>573.069229</td>\n",
       "      <td>1245.186170</td>\n",
       "      <td>573.367788</td>\n",
       "      <td>1244.715931</td>\n",
       "      <td>573.703307</td>\n",
       "      <td>...</td>\n",
       "      <td>578.924870</td>\n",
       "      <td>1250.390074</td>\n",
       "      <td>579.726618</td>\n",
       "      <td>1250.394318</td>\n",
       "      <td>579.359849</td>\n",
       "      <td>1249.968399</td>\n",
       "      <td>579.784303</td>\n",
       "      <td>1251.154055</td>\n",
       "      <td>580.416134</td>\n",
       "      <td>1251.291811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10028</td>\n",
       "      <td>1700.241874</td>\n",
       "      <td>316.086480</td>\n",
       "      <td>1700.525460</td>\n",
       "      <td>316.488920</td>\n",
       "      <td>1700.323302</td>\n",
       "      <td>316.890609</td>\n",
       "      <td>1701.103918</td>\n",
       "      <td>317.393865</td>\n",
       "      <td>1702.043964</td>\n",
       "      <td>...</td>\n",
       "      <td>1709.319240</td>\n",
       "      <td>324.937266</td>\n",
       "      <td>1708.664393</td>\n",
       "      <td>325.239303</td>\n",
       "      <td>1709.991220</td>\n",
       "      <td>325.661801</td>\n",
       "      <td>1710.131299</td>\n",
       "      <td>326.044896</td>\n",
       "      <td>1709.573802</td>\n",
       "      <td>326.235561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1003</td>\n",
       "      <td>2111.054734</td>\n",
       "      <td>675.383201</td>\n",
       "      <td>2108.569109</td>\n",
       "      <td>674.975255</td>\n",
       "      <td>2109.229033</td>\n",
       "      <td>674.075435</td>\n",
       "      <td>2108.321365</td>\n",
       "      <td>673.097262</td>\n",
       "      <td>2105.109658</td>\n",
       "      <td>...</td>\n",
       "      <td>2087.494021</td>\n",
       "      <td>660.523827</td>\n",
       "      <td>2088.360739</td>\n",
       "      <td>660.092050</td>\n",
       "      <td>2084.870704</td>\n",
       "      <td>659.015644</td>\n",
       "      <td>2085.411491</td>\n",
       "      <td>658.537079</td>\n",
       "      <td>2086.287679</td>\n",
       "      <td>658.085215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3195</th>\n",
       "      <td>9897</td>\n",
       "      <td>255.890604</td>\n",
       "      <td>805.689245</td>\n",
       "      <td>256.168465</td>\n",
       "      <td>805.650562</td>\n",
       "      <td>256.346062</td>\n",
       "      <td>805.646480</td>\n",
       "      <td>256.607315</td>\n",
       "      <td>805.495638</td>\n",
       "      <td>256.800517</td>\n",
       "      <td>...</td>\n",
       "      <td>261.586384</td>\n",
       "      <td>804.936101</td>\n",
       "      <td>261.802389</td>\n",
       "      <td>805.090005</td>\n",
       "      <td>262.007380</td>\n",
       "      <td>804.771894</td>\n",
       "      <td>262.185753</td>\n",
       "      <td>804.861020</td>\n",
       "      <td>262.504933</td>\n",
       "      <td>805.065756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3196</th>\n",
       "      <td>99</td>\n",
       "      <td>585.794282</td>\n",
       "      <td>1153.349509</td>\n",
       "      <td>585.975384</td>\n",
       "      <td>1153.354458</td>\n",
       "      <td>586.037156</td>\n",
       "      <td>1152.966003</td>\n",
       "      <td>586.233874</td>\n",
       "      <td>1152.309201</td>\n",
       "      <td>586.178494</td>\n",
       "      <td>...</td>\n",
       "      <td>588.930081</td>\n",
       "      <td>1148.492237</td>\n",
       "      <td>588.974821</td>\n",
       "      <td>1148.565545</td>\n",
       "      <td>589.194163</td>\n",
       "      <td>1148.095441</td>\n",
       "      <td>589.275727</td>\n",
       "      <td>1147.823628</td>\n",
       "      <td>589.819907</td>\n",
       "      <td>1148.155671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3197</th>\n",
       "      <td>9905</td>\n",
       "      <td>1759.216061</td>\n",
       "      <td>444.556901</td>\n",
       "      <td>1759.351763</td>\n",
       "      <td>444.932074</td>\n",
       "      <td>1759.062174</td>\n",
       "      <td>445.237485</td>\n",
       "      <td>1758.360328</td>\n",
       "      <td>445.547824</td>\n",
       "      <td>1758.130023</td>\n",
       "      <td>...</td>\n",
       "      <td>1753.131645</td>\n",
       "      <td>452.437027</td>\n",
       "      <td>1752.818720</td>\n",
       "      <td>452.611036</td>\n",
       "      <td>1752.950008</td>\n",
       "      <td>453.142609</td>\n",
       "      <td>1752.557617</td>\n",
       "      <td>453.319697</td>\n",
       "      <td>1751.935027</td>\n",
       "      <td>453.435647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3198</th>\n",
       "      <td>9910</td>\n",
       "      <td>574.463498</td>\n",
       "      <td>1289.823638</td>\n",
       "      <td>574.486791</td>\n",
       "      <td>1289.501212</td>\n",
       "      <td>574.313908</td>\n",
       "      <td>1289.067951</td>\n",
       "      <td>574.333836</td>\n",
       "      <td>1288.749067</td>\n",
       "      <td>574.467536</td>\n",
       "      <td>...</td>\n",
       "      <td>572.693804</td>\n",
       "      <td>1282.662556</td>\n",
       "      <td>572.708423</td>\n",
       "      <td>1282.348553</td>\n",
       "      <td>572.650770</td>\n",
       "      <td>1282.001251</td>\n",
       "      <td>572.421319</td>\n",
       "      <td>1282.006840</td>\n",
       "      <td>572.554907</td>\n",
       "      <td>1281.757542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3199</th>\n",
       "      <td>9918</td>\n",
       "      <td>582.501160</td>\n",
       "      <td>1163.757558</td>\n",
       "      <td>583.020060</td>\n",
       "      <td>1162.945143</td>\n",
       "      <td>582.632175</td>\n",
       "      <td>1162.157933</td>\n",
       "      <td>583.290079</td>\n",
       "      <td>1161.743147</td>\n",
       "      <td>583.054473</td>\n",
       "      <td>...</td>\n",
       "      <td>586.456615</td>\n",
       "      <td>1148.592502</td>\n",
       "      <td>586.847654</td>\n",
       "      <td>1146.722825</td>\n",
       "      <td>587.000915</td>\n",
       "      <td>1146.442607</td>\n",
       "      <td>586.883255</td>\n",
       "      <td>1145.984330</td>\n",
       "      <td>587.164991</td>\n",
       "      <td>1145.911610</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3200 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID           v1           v2           v3           v4           v5  \\\n",
       "0     10002  1735.154650   338.654799  1733.945627   338.942133  1735.310512   \n",
       "1     10015   726.124713  1230.547001   725.077134  1230.135992   725.860099   \n",
       "2     10019   572.624761  1245.055487   573.026634  1244.970067   573.069229   \n",
       "3     10028  1700.241874   316.086480  1700.525460   316.488920  1700.323302   \n",
       "4      1003  2111.054734   675.383201  2108.569109   674.975255  2109.229033   \n",
       "...     ...          ...          ...          ...          ...          ...   \n",
       "3195   9897   255.890604   805.689245   256.168465   805.650562   256.346062   \n",
       "3196     99   585.794282  1153.349509   585.975384  1153.354458   586.037156   \n",
       "3197   9905  1759.216061   444.556901  1759.351763   444.932074  1759.062174   \n",
       "3198   9910   574.463498  1289.823638   574.486791  1289.501212   574.313908   \n",
       "3199   9918   582.501160  1163.757558   583.020060  1162.945143   582.632175   \n",
       "\n",
       "               v6           v7           v8           v9  ...          v51  \\\n",
       "0      339.499853  1734.650127   340.105564  1736.692980  ...  1741.536977   \n",
       "1     1229.237264   725.632283  1229.980740   725.662231  ...   725.162504   \n",
       "2     1245.186170   573.367788  1244.715931   573.703307  ...   578.924870   \n",
       "3      316.890609  1701.103918   317.393865  1702.043964  ...  1709.319240   \n",
       "4      674.075435  2108.321365   673.097262  2105.109658  ...  2087.494021   \n",
       "...           ...          ...          ...          ...  ...          ...   \n",
       "3195   805.646480   256.607315   805.495638   256.800517  ...   261.586384   \n",
       "3196  1152.966003   586.233874  1152.309201   586.178494  ...   588.930081   \n",
       "3197   445.237485  1758.360328   445.547824  1758.130023  ...  1753.131645   \n",
       "3198  1289.067951   574.333836  1288.749067   574.467536  ...   572.693804   \n",
       "3199  1162.157933   583.290079  1161.743147   583.054473  ...   586.456615   \n",
       "\n",
       "              v52          v53          v54          v55          v56  \\\n",
       "0      349.360742  1743.828189   349.778961  1740.944639   350.426293   \n",
       "1     1224.285745   724.304063  1224.416170   724.419292  1223.590094   \n",
       "2     1250.390074   579.726618  1250.394318   579.359849  1249.968399   \n",
       "3      324.937266  1708.664393   325.239303  1709.991220   325.661801   \n",
       "4      660.523827  2088.360739   660.092050  2084.870704   659.015644   \n",
       "...           ...          ...          ...          ...          ...   \n",
       "3195   804.936101   261.802389   805.090005   262.007380   804.771894   \n",
       "3196  1148.492237   588.974821  1148.565545   589.194163  1148.095441   \n",
       "3197   452.437027  1752.818720   452.611036  1752.950008   453.142609   \n",
       "3198  1282.662556   572.708423  1282.348553   572.650770  1282.001251   \n",
       "3199  1148.592502   586.847654  1146.722825   587.000915  1146.442607   \n",
       "\n",
       "              v57          v58          v59          v60  \n",
       "0     1744.553706   350.962897  1741.523867   350.951357  \n",
       "1      724.567961  1224.685135   724.055162  1224.853199  \n",
       "2      579.784303  1251.154055   580.416134  1251.291811  \n",
       "3     1710.131299   326.044896  1709.573802   326.235561  \n",
       "4     2085.411491   658.537079  2086.287679   658.085215  \n",
       "...           ...          ...          ...          ...  \n",
       "3195   262.185753   804.861020   262.504933   805.065756  \n",
       "3196   589.275727  1147.823628   589.819907  1148.155671  \n",
       "3197  1752.557617   453.319697  1751.935027   453.435647  \n",
       "3198   572.421319  1282.006840   572.554907  1281.757542  \n",
       "3199   586.883255  1145.984330   587.164991  1145.911610  \n",
       "\n",
       "[3200 rows x 61 columns]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db08b55e-05e6-409e-b831-5f23ccd42ab3",
   "metadata": {
    "id": "db08b55e-05e6-409e-b831-5f23ccd42ab3"
   },
   "outputs": [],
   "source": [
    "# Ensemble Method "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
