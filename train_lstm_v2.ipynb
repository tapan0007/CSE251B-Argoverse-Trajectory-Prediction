{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea9f615f-1564-401f-9a74-8ac98f9bbeab",
   "metadata": {
    "id": "ea9f615f-1564-401f-9a74-8ac98f9bbeab",
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "import pickle\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8845100d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4f04ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"../train/train\"\n",
    "# The glob module finds all the pathnames matching a specified pattern\n",
    "train_pkl_lst = glob(os.path.join(train_path, '*'))\n",
    "#with open(train_pkl_lst[1], 'rb') as f:\n",
    "#    training_sample = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44b0f51-0779-49dc-bcab-5c284039d6fd",
   "metadata": {
    "id": "f44b0f51-0779-49dc-bcab-5c284039d6fd"
   },
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "68fac99f-9f5e-4fb5-ba6c-e22cc6f8f8be",
   "metadata": {
    "id": "68fac99f-9f5e-4fb5-ba6c-e22cc6f8f8be"
   },
   "outputs": [],
   "source": [
    "class MyLSTM(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, n_layers):\n",
    "        super(MyLSTM, self).__init__()\n",
    "\n",
    "        # Defining some parameters\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        #Defining the layers\n",
    "        # RNN Layer\n",
    "    \n",
    "        self.lstm = nn.LSTM(input_size, hidden_dim, n_layers, batch_first=True)   \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        #Initializing hidden state for first input using method defined below\n",
    "        h_t, c_t = self.init_hidden(batch_size)\n",
    "\n",
    "        #print(x.size())\n",
    "        #print(x.size(-1))\n",
    "        # Passing in the input and hidden state into the model and obtaining outputs\n",
    "        #print(x.shape)\n",
    "        #print(h_t.shape)\n",
    "        #print(c_t.shape)\n",
    "\n",
    "        #print(f'forward pass: input shape is {x.shape}')\n",
    "        out, (h_t, c_t) = self.lstm(x, (h_t, c_t))\n",
    "        #print(f'forward pass: lstm output is {out.shape}')\n",
    "        \n",
    "        # Reshaping the outputs such that it can be fit into the fully connected layer\n",
    "        out = out.contiguous().view(-1, self.hidden_dim)\n",
    "        #print(f'forward pass: result of reshaping output before passing to fc layer is {out.shape}')\n",
    "        out = self.fc(out)\n",
    "        #print(f'forward pass: fc output is {out.shape}')\n",
    "        \n",
    "        return out, h_t\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        # This method generates the first hidden state of zeros which we'll use in the forward pass\n",
    "        h_0 = torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(device)\n",
    "        c_0 =  torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(device)       \n",
    "         # We'll send the tensor holding the hidden state to the device we specified earlier as well\n",
    "        return h_0, c_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43901b7a-983c-49ca-b02a-e22d812f3cab",
   "metadata": {
    "id": "43901b7a-983c-49ca-b02a-e22d812f3cab"
   },
   "outputs": [],
   "source": [
    "# Autogressive vs. direct mapping\n",
    "# Batch Norm? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c020112-bb96-45d0-abf4-332956e8e544",
   "metadata": {
    "id": "1c020112-bb96-45d0-abf4-332956e8e544"
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "cc5190e9-67a3-4e83-af0c-2f79f9cab867",
   "metadata": {
    "id": "cc5190e9-67a3-4e83-af0c-2f79f9cab867"
   },
   "outputs": [],
   "source": [
    "class ArgoverseDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Dataset class for Argoverse\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 data_path,\n",
    "                 sample_indices):\n",
    "        super(ArgoverseDataset, self).__init__()\n",
    "        \n",
    "        self.data_path = data_path\n",
    "        self.sample_indices = sample_indices\n",
    "        self.pkl_list = glob(os.path.join(self.data_path, '*'))\n",
    "        self.pkl_list.sort()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sample_indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        # Load one scene\n",
    "        pkl_path = self.pkl_list[self.sample_indices[idx]]\n",
    "        with open(pkl_path, 'rb') as f:\n",
    "            scene = pickle.load(f)\n",
    "            \n",
    "        # the index of agent to be predicted \n",
    "        pred_id = np.where(scene[\"track_id\"] == scene['agent_id'])[0][0]\n",
    "        \n",
    "        # input: p_in & v_in; output: p_out\n",
    "        inp_scene = np.dstack([scene['p_in'], scene['v_in']])\n",
    "        out_scene = np.dstack([scene['p_out'], scene['v_out']])\n",
    "        \n",
    "        # Normalization \n",
    "        min_vecs = np.min(inp_scene, axis = (0,1))\n",
    "        max_vecs = np.max(inp_scene, axis = (0,1))\n",
    "        \n",
    "        # Normalize by vectors\n",
    "        inp = (inp_scene[pred_id] - min_vecs)/(max_vecs - min_vecs)\n",
    "        out = (out_scene[pred_id] - min_vecs)/(max_vecs - min_vecs)\n",
    "        \n",
    "        #print(inp.shape)\n",
    "        #print(inp.flatten().shape)\n",
    "        inp = inp.reshape(1, -1)\n",
    "        out = out[:, :2].reshape(-1) # reshape to 60x1 so it matches shape of model output\n",
    "        \n",
    "        #inp = inp.flatten()\n",
    "        #out = out[:, :2].flatten()\n",
    "        \n",
    "        #inp = inp.reshape(-1, 1)\n",
    "        #out = out[:, :2].reshape(-1, 1)\n",
    "        \n",
    "        return torch.from_numpy(inp).float(), torch.from_numpy(out).float()\n",
    "    \n",
    "        #dat = np.concatenate((inp, out), axis=0)\n",
    "        \n",
    "        #train_data = []\n",
    "        #window_size = 20\n",
    "        #interval = 7\n",
    "        #for i in range(0, len(dat), interval):\n",
    "        #    #print(len(dat[i:i+input_length]))\n",
    "        #    if i + window_size < len(dat): \n",
    "        #        train_data.append(dat[i:i+window_size])\n",
    "            \n",
    "        #print(len(train_data))\n",
    "        #print(train_data)\n",
    "        \n",
    "        #input_seq = []\n",
    "        #target_seq = []\n",
    "        #for i in range(len(train_data)):\n",
    "        #    input_seq.append(train_data[i][:-1])\n",
    "        #    target_seq.append(train_data[i][1:])\n",
    "        \n",
    "        #print(input_seq)\n",
    "        \n",
    "        #input_seq = np.array(input_seq, dtype=np.float32)\n",
    "        #target_seq = np.array(target_seq, dtype=np.float32)\n",
    "        \n",
    "        #print(input_seq.shape)\n",
    "        #print(target_seq.shape)\n",
    "        \n",
    "        ## Convert to float torch tensor\n",
    "        ##return torch.from_numpy(inp).float(), torch.from_numpy(out).float() #torch.from_numpy(out[:,:2]).float()\n",
    "        #return torch.from_numpy(input_seq).float(), torch.from_numpy(target_seq).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16e5f5c2-86ff-4a6b-a585-117d0d08b8b2",
   "metadata": {
    "id": "16e5f5c2-86ff-4a6b-a585-117d0d08b8b2"
   },
   "outputs": [],
   "source": [
    "# Try different ways of normalization\n",
    "# Leverage other features. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13ae604-526c-4fff-a672-f09cd103f7c2",
   "metadata": {
    "id": "c13ae604-526c-4fff-a672-f09cd103f7c2"
   },
   "source": [
    "# Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "913d832f-4050-4c23-9d74-114595112f13",
   "metadata": {
    "id": "913d832f-4050-4c23-9d74-114595112f13"
   },
   "outputs": [],
   "source": [
    "# Grid/Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "29e5c051-43a7-4bfd-9ab0-7105693661d3",
   "metadata": {
    "id": "29e5c051-43a7-4bfd-9ab0-7105693661d3"
   },
   "outputs": [],
   "source": [
    "# batch_size = 64\n",
    "interval = 7 # sampling interval for LSTM\n",
    "window_size = 20 # number of timesteps to take as input\n",
    "batch_size = 512\n",
    "#in_dim = 19*4 # MLP\n",
    "#out_dim = 4 #30*2 # MLP\n",
    "input_size = 19*4 #1#19*4 # LSTM\n",
    "output_size = 30*2 # LSTM (has to match input_size)\n",
    "hidden_dim = 32 #128 #256 #128 #32 #128\n",
    "num_layers = 1 #1 #3\n",
    "learning_rate = 0.01\n",
    "decay_rate = 0.95\n",
    "num_epoch = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7aa146-1618-4fc2-aec1-2724a09c8bd0",
   "metadata": {
    "id": "fb7aa146-1618-4fc2-aec1-2724a09c8bd0"
   },
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "d589c9c9-c46b-4b70-a515-90dd68669431",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d589c9c9-c46b-4b70-a515-90dd68669431",
    "outputId": "34773f5c-9441-4dd7-903b-970cb0e59e99"
   },
   "outputs": [],
   "source": [
    "train_path = \"../train/train\"\n",
    "\n",
    "# total number of scenes\n",
    "indices = np.arange(0, 205942)\n",
    "\n",
    "# train-valid split\n",
    "np.random.shuffle(indices)\n",
    "train_indices = indices[:180000]\n",
    "valid_indices = indices[180000:]\n",
    "\n",
    "# define datasets\n",
    "train_set = ArgoverseDataset(train_path, train_indices)\n",
    "valid_set = ArgoverseDataset(train_path, valid_indices)\n",
    "\n",
    "# create dataloaders\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_set, batch_size=batch_size, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "4ab0071e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "180000"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "629f8c9d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# train_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "6df2d6b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "352"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9c45bc-bdf3-4b5a-82b0-c0752c16cd2b",
   "metadata": {
    "id": "6d9c45bc-bdf3-4b5a-82b0-c0752c16cd2b"
   },
   "source": [
    "# Model, Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "8c3cc020-8055-4005-ba6b-585a2e7a5fee",
   "metadata": {
    "id": "8c3cc020-8055-4005-ba6b-585a2e7a5fee"
   },
   "outputs": [],
   "source": [
    "# # RNN, LSTM, 1dCNN, Transformer\n",
    "# model = MLPNet(in_dim = in_dim, \n",
    "#                out_dim = out_dim,\n",
    "#                hidden_dim = hidden_dim, \n",
    "#                num_layers = num_layers).to(device) # move model to gpu \n",
    "\n",
    "model = MyLSTM(input_size=input_size, output_size=output_size, hidden_dim=hidden_dim, n_layers=num_layers).to(device)\n",
    "\n",
    "# Adaptive Moment Estimation computes adaptive learning rates for each parameter. \n",
    "# Compute the decaying averages of past and past squared gradients. \n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=decay_rate)  # stepwise learning rate decay\n",
    "loss_fun = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3392ee-eb57-4d8d-b6ea-49e6f38e3770",
   "metadata": {
    "id": "1c3392ee-eb57-4d8d-b6ea-49e6f38e3770"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "795977ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 1, 76])\n",
      "torch.Size([512, 60])\n"
     ]
    }
   ],
   "source": [
    "for inp, tgt in train_loader:\n",
    "    print(inp.shape)\n",
    "    print(tgt.shape)\n",
    "    break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "fa3e350d-9fca-4356-84da-7412d86667c9",
   "metadata": {
    "id": "fa3e350d-9fca-4356-84da-7412d86667c9"
   },
   "outputs": [],
   "source": [
    "def train_epoch(train_loader, model, optimizer, loss_function):\n",
    "\n",
    "    train_mse = []\n",
    "    for inp, tgt in tqdm(train_loader):\n",
    "        \n",
    "        #inp = inp.view(-1, window_size-1, 4)\n",
    "        #tgt = tgt.view(-1, window_size-1, 4)\n",
    "        \n",
    "        #print(inp.size())\n",
    "        \n",
    "        #print(inp.shape)\n",
    "        #print(tgt.shape)\n",
    "        \n",
    "        inp = inp.to(device)\n",
    "        tgt = tgt.to(device)\n",
    "        \n",
    "        output, hidden = model(inp)\n",
    "        output = output.to(device)\n",
    "        \n",
    "        #print(output.shape)\n",
    "        #print(hidden.shape)\n",
    "        #print(output.shape)\n",
    "        #print(hidden.shape)\n",
    "        #print(tgt.view(-1, 4).size())\n",
    "        \n",
    "        \n",
    "        \n",
    "        loss = loss_function(output, tgt)\n",
    "        train_mse.append(loss.item()) \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    train_mse = round(np.sqrt(np.mean(train_mse)),5)\n",
    "    \n",
    "    return train_mse\n",
    "\n",
    "def eval_epoch(valid_loader, model, loss_function):\n",
    "    \n",
    "    valid_mse = []\n",
    "    #preds = []\n",
    "    #trues = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inp, tgt in valid_loader:\n",
    "            \n",
    "            #inp = inp.view(-1, window_size-1, 4)\n",
    "            #tgt = tgt.view(-1, window_size-1, 4)\n",
    "            \n",
    "            inp = inp.to(device)\n",
    "            tgt = tgt.to(device)\n",
    "            \n",
    "            loss = 0\n",
    "            output, hidden = model(inp)\n",
    "            output = output.to(device)\n",
    "                \n",
    "            loss = loss_function(output, tgt)\n",
    "            \n",
    "            #preds.append(pred.cpu().data.numpy())\n",
    "            #trues.append(tgt.cpu().data.numpy())\n",
    "            \n",
    "            valid_mse.append(loss.item())\n",
    "            \n",
    "        #preds = np.concatenate(preds, axis = 0)  \n",
    "        #trues = np.concatenate(trues, axis = 0)  \n",
    "        valid_mse = round(np.sqrt(np.mean(valid_mse)), 5)\n",
    "    return valid_mse#, preds, trues\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f529da48-7092-4096-b95d-a3782e0a7590",
   "metadata": {
    "id": "f529da48-7092-4096-b95d-a3782e0a7590"
   },
   "outputs": [],
   "source": [
    "# Learning Rate Decay\n",
    "# Dropout\n",
    "# L1/L2 Regulization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "3f7797c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "205942"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_pkl_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c15057-a83e-44b0-af97-cf28f3c98044",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 554
    },
    "id": "f5c15057-a83e-44b0-af97-cf28f3c98044",
    "outputId": "c23108e3-db6e-4d40-8242-42cf968e5c13",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]\n",
      "  0%|          | 0/352 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|          | 1/352 [00:00<04:43,  1.24it/s]\u001b[A\n",
      "  1%|          | 2/352 [00:01<04:41,  1.24it/s]\u001b[A\n",
      "  1%|          | 3/352 [00:02<04:35,  1.27it/s]\u001b[A\n",
      "  1%|          | 4/352 [00:03<04:29,  1.29it/s]\u001b[A\n",
      "  1%|▏         | 5/352 [00:03<04:29,  1.29it/s]\u001b[A\n",
      "  2%|▏         | 6/352 [00:04<04:32,  1.27it/s]\u001b[A\n",
      "  2%|▏         | 7/352 [00:05<04:29,  1.28it/s]\u001b[A\n",
      "  2%|▏         | 8/352 [00:06<04:29,  1.28it/s]\u001b[A\n",
      "  3%|▎         | 9/352 [00:07<04:22,  1.31it/s]\u001b[A\n",
      "  3%|▎         | 10/352 [00:07<04:09,  1.37it/s]\u001b[A\n",
      "  3%|▎         | 11/352 [00:08<04:03,  1.40it/s]\u001b[A\n",
      "  3%|▎         | 12/352 [00:09<04:09,  1.36it/s]\u001b[A\n",
      "  4%|▎         | 13/352 [00:09<04:03,  1.39it/s]\u001b[A\n",
      "  4%|▍         | 14/352 [00:10<04:01,  1.40it/s]\u001b[A\n",
      "  4%|▍         | 15/352 [00:11<04:02,  1.39it/s]\u001b[A\n",
      "  5%|▍         | 16/352 [00:11<04:00,  1.40it/s]\u001b[A\n",
      "  5%|▍         | 17/352 [00:12<03:55,  1.42it/s]\u001b[A\n",
      "  5%|▌         | 18/352 [00:13<03:55,  1.42it/s]\u001b[A\n",
      "  5%|▌         | 19/352 [00:14<04:06,  1.35it/s]\u001b[A\n",
      "  6%|▌         | 20/352 [00:14<04:10,  1.33it/s]\u001b[A\n",
      "  6%|▌         | 21/352 [00:15<04:13,  1.31it/s]\u001b[A\n",
      "  6%|▋         | 22/352 [00:16<04:13,  1.30it/s]\u001b[A\n",
      "  7%|▋         | 23/352 [00:17<04:15,  1.29it/s]\u001b[A\n",
      "  7%|▋         | 24/352 [00:18<04:11,  1.31it/s]\u001b[A\n",
      "  7%|▋         | 25/352 [00:18<04:05,  1.33it/s]\u001b[A\n",
      "  7%|▋         | 26/352 [00:19<04:01,  1.35it/s]\u001b[A\n",
      "  8%|▊         | 27/352 [00:20<03:57,  1.37it/s]\u001b[A\n",
      "  8%|▊         | 28/352 [00:20<03:53,  1.39it/s]\u001b[A\n",
      "  8%|▊         | 29/352 [00:21<03:47,  1.42it/s]\u001b[A\n",
      "  9%|▊         | 30/352 [00:22<03:46,  1.42it/s]\u001b[A\n",
      "  9%|▉         | 31/352 [00:22<03:45,  1.42it/s]\u001b[A\n",
      "  9%|▉         | 32/352 [00:23<03:52,  1.37it/s]\u001b[A\n",
      "  9%|▉         | 33/352 [00:24<03:56,  1.35it/s]\u001b[A\n",
      " 10%|▉         | 34/352 [00:25<03:51,  1.37it/s]\u001b[A\n",
      " 10%|▉         | 35/352 [00:25<03:48,  1.39it/s]\u001b[A\n",
      " 10%|█         | 36/352 [00:26<03:45,  1.40it/s]\u001b[A\n",
      " 11%|█         | 37/352 [00:27<03:44,  1.41it/s]\u001b[A\n",
      " 11%|█         | 38/352 [00:28<03:47,  1.38it/s]\u001b[A\n",
      " 11%|█         | 39/352 [00:28<03:45,  1.39it/s]\u001b[A\n",
      " 11%|█▏        | 40/352 [00:29<03:43,  1.39it/s]\u001b[A\n",
      " 12%|█▏        | 41/352 [00:30<03:37,  1.43it/s]\u001b[A\n",
      " 12%|█▏        | 42/352 [00:30<03:40,  1.41it/s]\u001b[A\n",
      " 12%|█▏        | 43/352 [00:31<03:37,  1.42it/s]\u001b[A\n",
      " 12%|█▎        | 44/352 [00:32<03:37,  1.41it/s]\u001b[A\n",
      " 13%|█▎        | 45/352 [00:32<03:38,  1.41it/s]\u001b[A\n",
      " 13%|█▎        | 46/352 [00:33<03:40,  1.39it/s]\u001b[A\n",
      " 13%|█▎        | 47/352 [00:34<03:45,  1.35it/s]\u001b[A\n",
      " 14%|█▎        | 48/352 [00:35<03:45,  1.35it/s]\u001b[A\n",
      " 14%|█▍        | 49/352 [00:36<03:50,  1.31it/s]\u001b[A\n",
      " 14%|█▍        | 50/352 [00:36<03:46,  1.33it/s]\u001b[A\n",
      " 14%|█▍        | 51/352 [00:37<03:44,  1.34it/s]\u001b[A\n",
      " 15%|█▍        | 52/352 [00:38<03:44,  1.34it/s]\u001b[A\n",
      " 15%|█▌        | 53/352 [00:39<03:49,  1.30it/s]\u001b[A\n",
      " 15%|█▌        | 54/352 [00:39<03:52,  1.28it/s]\u001b[A\n",
      " 16%|█▌        | 55/352 [00:40<03:55,  1.26it/s]\u001b[A\n",
      " 16%|█▌        | 56/352 [00:41<03:55,  1.26it/s]\u001b[A\n",
      " 16%|█▌        | 57/352 [00:42<03:58,  1.24it/s]\u001b[A\n",
      " 16%|█▋        | 58/352 [00:43<03:50,  1.27it/s]\u001b[A\n",
      " 17%|█▋        | 59/352 [00:43<03:43,  1.31it/s]\u001b[A\n",
      " 17%|█▋        | 60/352 [00:44<03:39,  1.33it/s]\u001b[A\n",
      " 17%|█▋        | 61/352 [00:45<03:43,  1.30it/s]\u001b[A\n",
      " 18%|█▊        | 62/352 [00:46<03:45,  1.29it/s]\u001b[A\n",
      " 18%|█▊        | 63/352 [00:46<03:45,  1.28it/s]\u001b[A\n",
      " 18%|█▊        | 64/352 [00:47<03:43,  1.29it/s]\u001b[A\n",
      " 18%|█▊        | 65/352 [00:48<03:45,  1.27it/s]\u001b[A\n",
      " 19%|█▉        | 66/352 [00:49<03:49,  1.25it/s]\u001b[A\n",
      " 19%|█▉        | 67/352 [00:50<03:48,  1.25it/s]\u001b[A\n",
      " 19%|█▉        | 68/352 [00:50<03:41,  1.28it/s]\u001b[A\n",
      " 20%|█▉        | 69/352 [00:51<04:08,  1.14it/s]\u001b[A\n",
      " 20%|█▉        | 70/352 [00:52<03:58,  1.18it/s]\u001b[A\n",
      " 20%|██        | 71/352 [00:53<03:57,  1.18it/s]\u001b[A\n",
      " 20%|██        | 72/352 [00:54<03:55,  1.19it/s]\u001b[A\n",
      " 21%|██        | 73/352 [00:55<03:50,  1.21it/s]\u001b[A\n",
      " 21%|██        | 74/352 [00:56<03:47,  1.22it/s]\u001b[A\n",
      " 21%|██▏       | 75/352 [00:56<03:43,  1.24it/s]\u001b[A\n",
      " 22%|██▏       | 76/352 [00:57<03:35,  1.28it/s]\u001b[A\n",
      " 22%|██▏       | 77/352 [00:58<03:33,  1.29it/s]\u001b[A\n",
      " 22%|██▏       | 78/352 [00:59<03:36,  1.27it/s]\u001b[A\n",
      " 22%|██▏       | 79/352 [00:59<03:36,  1.26it/s]\u001b[A\n",
      " 23%|██▎       | 80/352 [01:00<03:36,  1.26it/s]\u001b[A\n",
      " 23%|██▎       | 81/352 [01:01<03:36,  1.25it/s]\u001b[A\n",
      " 23%|██▎       | 82/352 [01:02<03:30,  1.28it/s]\u001b[A\n",
      " 24%|██▎       | 83/352 [01:03<03:30,  1.28it/s]\u001b[A\n",
      " 24%|██▍       | 84/352 [01:03<03:29,  1.28it/s]\u001b[A\n",
      " 24%|██▍       | 85/352 [01:04<03:26,  1.30it/s]\u001b[A\n",
      " 24%|██▍       | 86/352 [01:05<03:25,  1.29it/s]\u001b[A\n",
      " 25%|██▍       | 87/352 [01:06<03:21,  1.32it/s]\u001b[A\n",
      " 25%|██▌       | 88/352 [01:06<03:23,  1.29it/s]\u001b[A\n",
      " 25%|██▌       | 89/352 [01:07<03:26,  1.27it/s]\u001b[A\n",
      " 26%|██▌       | 90/352 [01:08<03:27,  1.26it/s]\u001b[A\n",
      " 26%|██▌       | 91/352 [01:09<03:32,  1.23it/s]\u001b[A\n",
      " 26%|██▌       | 92/352 [01:10<03:31,  1.23it/s]\u001b[A\n",
      " 26%|██▋       | 93/352 [01:10<03:26,  1.26it/s]\u001b[A\n",
      " 27%|██▋       | 94/352 [01:11<03:24,  1.26it/s]\u001b[A\n",
      " 27%|██▋       | 95/352 [01:12<03:20,  1.28it/s]\u001b[A\n",
      " 27%|██▋       | 96/352 [01:13<03:22,  1.26it/s]\u001b[A\n",
      " 28%|██▊       | 97/352 [01:14<03:23,  1.26it/s]\u001b[A\n",
      " 28%|██▊       | 98/352 [01:14<03:22,  1.25it/s]\u001b[A\n",
      " 28%|██▊       | 99/352 [01:15<03:18,  1.27it/s]\u001b[A\n",
      " 28%|██▊       | 100/352 [01:16<03:15,  1.29it/s]\u001b[A\n",
      " 29%|██▊       | 101/352 [01:17<03:07,  1.34it/s]\u001b[A\n",
      " 29%|██▉       | 102/352 [01:17<03:04,  1.36it/s]\u001b[A\n",
      " 29%|██▉       | 103/352 [01:18<03:07,  1.33it/s]\u001b[A\n",
      " 30%|██▉       | 104/352 [01:19<03:08,  1.32it/s]\u001b[A\n",
      " 30%|██▉       | 105/352 [01:20<03:12,  1.28it/s]\u001b[A\n",
      " 30%|███       | 106/352 [01:21<03:13,  1.27it/s]\u001b[A\n",
      " 30%|███       | 107/352 [01:21<03:13,  1.26it/s]\u001b[A\n",
      " 31%|███       | 108/352 [01:22<03:14,  1.26it/s]\u001b[A\n",
      " 31%|███       | 109/352 [01:23<03:10,  1.27it/s]\u001b[A\n",
      " 31%|███▏      | 110/352 [01:24<03:08,  1.28it/s]\u001b[A\n",
      " 32%|███▏      | 111/352 [01:24<03:05,  1.30it/s]\u001b[A\n",
      " 32%|███▏      | 112/352 [01:25<03:02,  1.32it/s]\u001b[A\n",
      " 32%|███▏      | 113/352 [01:26<03:04,  1.30it/s]\u001b[A\n",
      " 32%|███▏      | 114/352 [01:27<03:01,  1.31it/s]\u001b[A\n",
      " 33%|███▎      | 115/352 [01:27<03:00,  1.32it/s]\u001b[A\n",
      " 33%|███▎      | 116/352 [01:28<03:04,  1.28it/s]\u001b[A\n",
      " 33%|███▎      | 117/352 [01:29<03:06,  1.26it/s]\u001b[A\n",
      " 34%|███▎      | 118/352 [01:30<03:06,  1.25it/s]\u001b[A\n",
      " 34%|███▍      | 119/352 [01:31<03:06,  1.25it/s]\u001b[A\n",
      " 34%|███▍      | 120/352 [01:32<03:08,  1.23it/s]\u001b[A\n",
      " 34%|███▍      | 121/352 [01:32<03:07,  1.23it/s]\u001b[A\n",
      " 35%|███▍      | 122/352 [01:33<03:09,  1.21it/s]\u001b[A\n",
      " 35%|███▍      | 123/352 [01:34<03:07,  1.22it/s]\u001b[A\n",
      " 35%|███▌      | 124/352 [01:35<03:04,  1.24it/s]\u001b[A\n",
      " 36%|███▌      | 125/352 [01:36<02:57,  1.28it/s]\u001b[A\n",
      " 36%|███▌      | 126/352 [01:36<02:58,  1.26it/s]\u001b[A\n",
      " 36%|███▌      | 127/352 [01:37<03:01,  1.24it/s]\u001b[A\n",
      " 36%|███▋      | 128/352 [01:38<02:55,  1.28it/s]\u001b[A\n",
      " 37%|███▋      | 129/352 [01:39<02:51,  1.30it/s]\u001b[A\n",
      " 37%|███▋      | 130/352 [01:39<02:49,  1.31it/s]\u001b[A\n",
      " 37%|███▋      | 131/352 [01:40<02:44,  1.34it/s]\u001b[A\n",
      " 38%|███▊      | 132/352 [01:41<02:42,  1.35it/s]\u001b[A\n",
      " 38%|███▊      | 133/352 [01:42<02:42,  1.35it/s]\u001b[A\n",
      " 38%|███▊      | 134/352 [01:42<02:42,  1.34it/s]\u001b[A\n",
      " 38%|███▊      | 135/352 [01:43<02:42,  1.34it/s]\u001b[A\n",
      " 39%|███▊      | 136/352 [01:44<02:39,  1.35it/s]\u001b[A\n",
      " 39%|███▉      | 137/352 [01:44<02:37,  1.36it/s]\u001b[A\n",
      " 39%|███▉      | 138/352 [01:45<02:36,  1.36it/s]\u001b[A\n",
      " 39%|███▉      | 139/352 [01:46<02:40,  1.33it/s]\u001b[A\n",
      " 40%|███▉      | 140/352 [01:47<02:39,  1.33it/s]\u001b[A\n",
      " 40%|████      | 141/352 [01:47<02:37,  1.34it/s]\u001b[A\n",
      " 40%|████      | 142/352 [01:48<02:36,  1.34it/s]\u001b[A\n",
      " 41%|████      | 143/352 [01:49<02:39,  1.31it/s]\u001b[A\n",
      " 41%|████      | 144/352 [01:50<02:43,  1.28it/s]\u001b[A\n",
      " 41%|████      | 145/352 [01:51<02:48,  1.23it/s]\u001b[A\n",
      " 41%|████▏     | 146/352 [01:52<02:49,  1.21it/s]\u001b[A\n",
      " 42%|████▏     | 147/352 [01:52<02:46,  1.23it/s]\u001b[A\n",
      " 42%|████▏     | 148/352 [01:53<02:43,  1.25it/s]\u001b[A\n",
      " 42%|████▏     | 149/352 [01:54<02:39,  1.28it/s]\u001b[A\n",
      " 43%|████▎     | 150/352 [01:55<02:34,  1.31it/s]\u001b[A\n",
      " 43%|████▎     | 151/352 [01:55<02:30,  1.34it/s]\u001b[A\n",
      " 43%|████▎     | 152/352 [01:56<02:26,  1.36it/s]\u001b[A\n",
      " 43%|████▎     | 153/352 [01:57<02:27,  1.35it/s]\u001b[A\n",
      " 44%|████▍     | 154/352 [01:58<02:27,  1.34it/s]\u001b[A\n",
      " 44%|████▍     | 155/352 [01:58<02:27,  1.33it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 156/352 [01:59<02:27,  1.33it/s]\u001b[A\n",
      " 45%|████▍     | 157/352 [02:00<02:24,  1.35it/s]\u001b[A\n",
      " 45%|████▍     | 158/352 [02:01<02:22,  1.36it/s]\u001b[A\n",
      " 45%|████▌     | 159/352 [02:01<02:20,  1.37it/s]\u001b[A\n",
      " 45%|████▌     | 160/352 [02:02<02:18,  1.38it/s]\u001b[A\n",
      " 46%|████▌     | 161/352 [02:03<02:16,  1.40it/s]\u001b[A\n",
      " 46%|████▌     | 162/352 [02:03<02:11,  1.44it/s]\u001b[A\n",
      " 46%|████▋     | 163/352 [02:04<02:06,  1.49it/s]\u001b[A\n",
      " 47%|████▋     | 164/352 [02:05<02:07,  1.48it/s]\u001b[A\n",
      " 47%|████▋     | 165/352 [02:05<02:02,  1.52it/s]\u001b[A\n",
      " 47%|████▋     | 166/352 [02:06<02:00,  1.54it/s]\u001b[A\n",
      " 47%|████▋     | 167/352 [02:07<02:02,  1.51it/s]\u001b[A\n",
      " 48%|████▊     | 168/352 [02:07<02:01,  1.52it/s]\u001b[A\n",
      " 48%|████▊     | 169/352 [02:08<02:01,  1.51it/s]\u001b[A\n",
      " 48%|████▊     | 170/352 [02:08<01:59,  1.53it/s]\u001b[A\n",
      " 49%|████▊     | 171/352 [02:09<02:17,  1.32it/s]\u001b[A\n",
      " 49%|████▉     | 172/352 [02:10<02:12,  1.36it/s]\u001b[A\n",
      " 49%|████▉     | 173/352 [02:11<02:06,  1.41it/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "train_rmse = []\n",
    "valid_rmse = []\n",
    "min_rmse = 10e8\n",
    "\n",
    "for i in tqdm(range(num_epoch)):\n",
    "    start = time.time()\n",
    "\n",
    "    model.train() # if you use dropout or batchnorm. \n",
    "    train_rmse.append(train_epoch(train_loader, model, optimizer, loss_fun))\n",
    "    print(train_rmse)\n",
    "    \n",
    "    model.eval()\n",
    "    val_rmse = eval_epoch(valid_loader, model, loss_fun)\n",
    "    valid_rmse.append(val_rmse)\n",
    "    print(val_rmse)\n",
    "\n",
    "    # save the best model\n",
    "    if valid_rmse[-1] < min_rmse:\n",
    "        min_rmse = valid_rmse[-1] \n",
    "        best_model = model\n",
    "        \n",
    "        # torch.save([best_model, i, get_lr(optimizer)], name + \".pth\")\n",
    "\n",
    "    end = time.time()\n",
    "    \n",
    "    # Early Stopping\n",
    "    if (len(train_rmse) > 100 and np.mean(valid_rmse[-5:]) >= np.mean(valid_rmse[-10:-5])):\n",
    "        torch.save(best_model.state_dict(), f'lstm_hdim_{hidden_dim}_wsize_{window_size}_interval_{interval}_nlayers_{num_layers}_bs_{batch_size}_lr_{learning_rate}_decay_{decay_rate}_epoch_{i+1}.pt')    \n",
    "        break       \n",
    "\n",
    "    # Learning Rate Decay        \n",
    "    scheduler.step()\n",
    "    \n",
    "    print(\"Epoch {} | T: {:0.2f} | Train RMSE: {:0.5f} | Valid RMSE: {:0.5f}\".format(i + 1, (end-start) / 60, train_rmse[-1], valid_rmse[-1]))\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(train_rmse, label=\"train_rmse\")\n",
    "    plt.plot(valid_rmse, label=\"valid_rmse\")\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('RSME loss')\n",
    "    plt.title(f'RMSE loss curve for LSTM, hdim: {hidden_dim}, wsize: {window_size}, nlayers: {num_layers}, bs: {batch_size}, lr: {learning_rate}, decay: {decay_rate}')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'lstm_loss_curve_v1_hdim_{hidden_dim}_wsize_{window_size}_interval_{interval}_nlayers_{num_layers}_bs_{batch_size}_lr_{learning_rate}_decay_{decay_rate}.png')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540da579-31a5-462c-826b-e8e1c1a2ddd9",
   "metadata": {
    "id": "540da579-31a5-462c-826b-e8e1c1a2ddd9"
   },
   "source": [
    "# Evaluation and Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f90248e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('lstm_hdim_32_wsize_20_interval_7_nlayers_1_bs_512_lr_0.01_decay_0.95.pt'))\n",
    "best_model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "cef337bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyLSTM(\n",
       "  (lstm): LSTM(4, 128, num_layers=2, batch_first=True)\n",
       "  (fc): Linear(in_features=128, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "66654867-c45e-4ad2-bdb4-ce1d546ea2d2",
   "metadata": {
    "id": "66654867-c45e-4ad2-bdb4-ce1d546ea2d2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_path = \"../val_in/val_in/\"\n",
    "test_pkl_list = glob(os.path.join(test_path, '*'))\n",
    "test_pkl_list.sort()\n",
    "\n",
    "test_preds = []\n",
    "for idx in range(len(test_pkl_list)):\n",
    "    with open(test_pkl_list[idx], 'rb') as f:\n",
    "        test_sample = pickle.load(f)\n",
    "        pred_id = np.where(test_sample[\"track_id\"] == test_sample['agent_id'])[0][0]\n",
    "        inp_scene = np.dstack([test_sample['p_in'], test_sample['v_in']])\n",
    "\n",
    "        # Normalization \n",
    "        min_vecs = np.min(inp_scene, axis = (0,1))\n",
    "        max_vecs = np.max(inp_scene, axis = (0,1))\n",
    "        #print(min_vecs.shape)\n",
    "        #print(max_vecs.shape)\n",
    "        \n",
    "        inp = (inp_scene[pred_id] - min_vecs)/(max_vecs - min_vecs)\n",
    "        \n",
    "        inp = torch.from_numpy(inp).float().to(device).unsqueeze(0)\n",
    "\n",
    "        #print(inp)\n",
    "        # post-processing for LSTM\n",
    "        predictions = [[]]\n",
    "        inp_data = inp[0][-1]\n",
    "        #print(inp_data.size())\n",
    "        for i in range(30):\n",
    "            preds = best_model(inp_data.reshape(1, 1, 4))\n",
    "            predictions[0].append(preds[0].cpu().data.numpy()[0, :2])\n",
    "            #print(preds)\n",
    "            inp_data = preds[0]\n",
    "            \n",
    "#         print(inp[0][-1])\n",
    "#         preds = best_model(inp)#.cpu().data.numpy()\n",
    "#         print(preds)\n",
    "#         print(inp.shape)\n",
    "#         print(preds[0].shape)\n",
    "\n",
    "        predictions = np.array(predictions)\n",
    "        #print(predictions.shape)\n",
    "\n",
    "        # De-Normalization ! \n",
    "        predictions = predictions * (max_vecs[:2] - min_vecs[:2]) +  min_vecs[:2]\n",
    "        test_preds.append(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "db582d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 30, 2)\n",
      "[[[1719.4099948   336.36949324]\n",
      "  [1725.20331361  336.28454933]\n",
      "  [1731.18027481  336.08865915]\n",
      "  [1737.28438104  335.81118443]\n",
      "  [1743.46870725  335.4714306 ]\n",
      "  [1749.69527642  335.08248608]\n",
      "  [1755.93245834  334.65360004]\n",
      "  [1762.15403327  334.19208017]\n",
      "  [1768.33648664  333.70396898]\n",
      "  [1774.45859294  333.19495993]\n",
      "  [1780.5009995   332.67050654]\n",
      "  [1786.44518599  332.13593142]\n",
      "  [1792.27408876  331.59644814]\n",
      "  [1797.97168464  331.05707393]\n",
      "  [1803.52215851  330.52256425]\n",
      "  [1808.91240051  329.99732555]\n",
      "  [1814.12950884  329.48530616]\n",
      "  [1819.16245456  328.98986545]\n",
      "  [1824.00187347  328.51368654]\n",
      "  [1828.6402742   328.05881997]\n",
      "  [1833.07141396  327.62664   ]\n",
      "  [1837.29113087  327.21788833]\n",
      "  [1841.29734401  326.83256494]\n",
      "  [1845.08838863  326.47021175]\n",
      "  [1848.6650971   326.12975987]\n",
      "  [1852.02955035  325.80992226]\n",
      "  [1855.18507788  325.50908469]\n",
      "  [1858.13667393  325.22574199]\n",
      "  [1860.8907894   324.95851988]\n",
      "  [1863.4559561   324.70658941]]]\n"
     ]
    }
   ],
   "source": [
    "print(test_preds[0].shape)\n",
    "print(test_preds[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f945f2f3-2b73-471f-91e5-87c63eb06a77",
   "metadata": {
    "id": "f945f2f3-2b73-471f-91e5-87c63eb06a77"
   },
   "source": [
    "# Generate Submission File"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec024e36",
   "metadata": {},
   "source": [
    "### Steps to create submission file \n",
    "Run the below cells. The last cell will generate a submission file \"test_submission.csv\" that you can submit to Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "af9c27f8-a65b-48ce-861b-262c3f0422e6",
   "metadata": {
    "id": "af9c27f8-a65b-48ce-861b-262c3f0422e6"
   },
   "outputs": [],
   "source": [
    "# Submission Files\n",
    "sample_sub = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6b504543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save for later use\n",
    "predictions = np.concatenate(test_preds).reshape(len(test_preds), -1)#.astype(int)\n",
    "sub_df = pd.DataFrame(np.c_[sample_sub[\"ID\"], predictions], columns=[np.r_[[\"ID\"], [\"v\" + str(i) for i in range(1, 61)]]])\n",
    "sub_df[\"ID\"] = sub_df[\"ID\"].astype(int)\n",
    "sub_df.to_csv(f'test_submission_lstm_hdim_{hidden_dim}_wsize_{window_size}_interval_{interval}_nlayers_{num_layers}_bs_{batch_size}_lr_{learning_rate}_decay_{decay_rate}.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f01524a4-5473-4c1f-9991-46fd62162e20",
   "metadata": {
    "id": "f01524a4-5473-4c1f-9991-46fd62162e20"
   },
   "outputs": [],
   "source": [
    "# Convert to float\n",
    "predictions = np.concatenate(test_preds).reshape(len(test_preds), -1)\n",
    "sub_df = pd.DataFrame(np.c_[sample_sub[\"ID\"], predictions], columns=[np.r_[[\"ID\"], [\"v\" + str(i) for i in range(1, 61)]]])\n",
    "sub_df[\"ID\"] = sub_df[\"ID\"].astype(int)\n",
    "sub_df.to_csv('test_submission.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "282ca735",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>v3</th>\n",
       "      <th>v4</th>\n",
       "      <th>v5</th>\n",
       "      <th>v6</th>\n",
       "      <th>v7</th>\n",
       "      <th>v8</th>\n",
       "      <th>v9</th>\n",
       "      <th>...</th>\n",
       "      <th>v51</th>\n",
       "      <th>v52</th>\n",
       "      <th>v53</th>\n",
       "      <th>v54</th>\n",
       "      <th>v55</th>\n",
       "      <th>v56</th>\n",
       "      <th>v57</th>\n",
       "      <th>v58</th>\n",
       "      <th>v59</th>\n",
       "      <th>v60</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10002</td>\n",
       "      <td>1719.409995</td>\n",
       "      <td>336.369493</td>\n",
       "      <td>1725.203314</td>\n",
       "      <td>336.284549</td>\n",
       "      <td>1731.180275</td>\n",
       "      <td>336.088659</td>\n",
       "      <td>1737.284381</td>\n",
       "      <td>335.811184</td>\n",
       "      <td>1743.468707</td>\n",
       "      <td>...</td>\n",
       "      <td>1852.029550</td>\n",
       "      <td>325.809922</td>\n",
       "      <td>1855.185078</td>\n",
       "      <td>325.509085</td>\n",
       "      <td>1858.136674</td>\n",
       "      <td>325.225742</td>\n",
       "      <td>1860.890789</td>\n",
       "      <td>324.958520</td>\n",
       "      <td>1863.455956</td>\n",
       "      <td>324.706589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10015</td>\n",
       "      <td>727.496177</td>\n",
       "      <td>1228.487343</td>\n",
       "      <td>729.501623</td>\n",
       "      <td>1227.020683</td>\n",
       "      <td>731.551209</td>\n",
       "      <td>1225.541131</td>\n",
       "      <td>733.649858</td>\n",
       "      <td>1224.036116</td>\n",
       "      <td>735.807510</td>\n",
       "      <td>...</td>\n",
       "      <td>787.935138</td>\n",
       "      <td>1192.248323</td>\n",
       "      <td>789.823385</td>\n",
       "      <td>1191.082460</td>\n",
       "      <td>791.612788</td>\n",
       "      <td>1189.971742</td>\n",
       "      <td>793.303437</td>\n",
       "      <td>1188.914976</td>\n",
       "      <td>794.896137</td>\n",
       "      <td>1187.909617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10019</td>\n",
       "      <td>575.937420</td>\n",
       "      <td>1245.525411</td>\n",
       "      <td>577.949851</td>\n",
       "      <td>1246.229434</td>\n",
       "      <td>579.888580</td>\n",
       "      <td>1246.909017</td>\n",
       "      <td>581.766447</td>\n",
       "      <td>1247.556617</td>\n",
       "      <td>583.588199</td>\n",
       "      <td>...</td>\n",
       "      <td>618.134380</td>\n",
       "      <td>1208.988457</td>\n",
       "      <td>619.759209</td>\n",
       "      <td>1205.888133</td>\n",
       "      <td>621.359993</td>\n",
       "      <td>1202.839988</td>\n",
       "      <td>622.930811</td>\n",
       "      <td>1199.866418</td>\n",
       "      <td>624.466382</td>\n",
       "      <td>1196.988248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10028</td>\n",
       "      <td>1688.421868</td>\n",
       "      <td>315.642609</td>\n",
       "      <td>1686.011086</td>\n",
       "      <td>315.433464</td>\n",
       "      <td>1683.885100</td>\n",
       "      <td>315.277506</td>\n",
       "      <td>1682.060234</td>\n",
       "      <td>315.193592</td>\n",
       "      <td>1680.518450</td>\n",
       "      <td>...</td>\n",
       "      <td>1658.053798</td>\n",
       "      <td>317.807724</td>\n",
       "      <td>1656.362865</td>\n",
       "      <td>318.090679</td>\n",
       "      <td>1654.566922</td>\n",
       "      <td>318.396584</td>\n",
       "      <td>1652.656799</td>\n",
       "      <td>318.726605</td>\n",
       "      <td>1650.622822</td>\n",
       "      <td>319.082017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1003</td>\n",
       "      <td>2124.988001</td>\n",
       "      <td>677.337634</td>\n",
       "      <td>2126.253720</td>\n",
       "      <td>676.509473</td>\n",
       "      <td>2127.818240</td>\n",
       "      <td>675.686718</td>\n",
       "      <td>2129.642917</td>\n",
       "      <td>674.856144</td>\n",
       "      <td>2131.708043</td>\n",
       "      <td>...</td>\n",
       "      <td>2218.717961</td>\n",
       "      <td>644.154731</td>\n",
       "      <td>2224.051100</td>\n",
       "      <td>642.274036</td>\n",
       "      <td>2229.361461</td>\n",
       "      <td>640.417128</td>\n",
       "      <td>2234.624730</td>\n",
       "      <td>638.597277</td>\n",
       "      <td>2239.818130</td>\n",
       "      <td>636.827166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3195</th>\n",
       "      <td>9897</td>\n",
       "      <td>256.107934</td>\n",
       "      <td>806.429025</td>\n",
       "      <td>256.087218</td>\n",
       "      <td>807.626338</td>\n",
       "      <td>256.094916</td>\n",
       "      <td>809.096276</td>\n",
       "      <td>256.088205</td>\n",
       "      <td>810.722548</td>\n",
       "      <td>256.036828</td>\n",
       "      <td>...</td>\n",
       "      <td>240.068192</td>\n",
       "      <td>856.978647</td>\n",
       "      <td>238.445174</td>\n",
       "      <td>858.486782</td>\n",
       "      <td>236.737045</td>\n",
       "      <td>859.748825</td>\n",
       "      <td>234.947427</td>\n",
       "      <td>860.738632</td>\n",
       "      <td>233.081285</td>\n",
       "      <td>861.434141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3196</th>\n",
       "      <td>99</td>\n",
       "      <td>588.224553</td>\n",
       "      <td>1155.657332</td>\n",
       "      <td>588.625253</td>\n",
       "      <td>1156.738643</td>\n",
       "      <td>589.040805</td>\n",
       "      <td>1157.748837</td>\n",
       "      <td>589.466163</td>\n",
       "      <td>1158.689328</td>\n",
       "      <td>589.894316</td>\n",
       "      <td>...</td>\n",
       "      <td>595.922750</td>\n",
       "      <td>1171.919051</td>\n",
       "      <td>596.053556</td>\n",
       "      <td>1172.410363</td>\n",
       "      <td>596.170164</td>\n",
       "      <td>1172.903513</td>\n",
       "      <td>596.272536</td>\n",
       "      <td>1173.400056</td>\n",
       "      <td>596.360345</td>\n",
       "      <td>1173.901477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3197</th>\n",
       "      <td>9905</td>\n",
       "      <td>1759.557891</td>\n",
       "      <td>444.402348</td>\n",
       "      <td>1763.731861</td>\n",
       "      <td>444.406015</td>\n",
       "      <td>1767.959124</td>\n",
       "      <td>444.195250</td>\n",
       "      <td>1772.224438</td>\n",
       "      <td>443.849696</td>\n",
       "      <td>1776.542309</td>\n",
       "      <td>...</td>\n",
       "      <td>1866.443332</td>\n",
       "      <td>422.076754</td>\n",
       "      <td>1869.704598</td>\n",
       "      <td>421.300012</td>\n",
       "      <td>1872.801255</td>\n",
       "      <td>420.583009</td>\n",
       "      <td>1875.730781</td>\n",
       "      <td>419.922832</td>\n",
       "      <td>1878.491704</td>\n",
       "      <td>419.315732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3198</th>\n",
       "      <td>9910</td>\n",
       "      <td>576.237940</td>\n",
       "      <td>1291.916656</td>\n",
       "      <td>577.363511</td>\n",
       "      <td>1294.875644</td>\n",
       "      <td>578.157823</td>\n",
       "      <td>1297.933344</td>\n",
       "      <td>578.671536</td>\n",
       "      <td>1301.083615</td>\n",
       "      <td>578.939946</td>\n",
       "      <td>...</td>\n",
       "      <td>541.070220</td>\n",
       "      <td>1382.560169</td>\n",
       "      <td>537.053684</td>\n",
       "      <td>1384.583994</td>\n",
       "      <td>532.847029</td>\n",
       "      <td>1386.130476</td>\n",
       "      <td>528.465023</td>\n",
       "      <td>1387.168444</td>\n",
       "      <td>523.925801</td>\n",
       "      <td>1387.676014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3199</th>\n",
       "      <td>9918</td>\n",
       "      <td>587.638949</td>\n",
       "      <td>1166.481728</td>\n",
       "      <td>589.380660</td>\n",
       "      <td>1168.105146</td>\n",
       "      <td>590.439682</td>\n",
       "      <td>1170.143350</td>\n",
       "      <td>591.029620</td>\n",
       "      <td>1172.422023</td>\n",
       "      <td>591.255681</td>\n",
       "      <td>...</td>\n",
       "      <td>543.506071</td>\n",
       "      <td>1214.570876</td>\n",
       "      <td>539.476751</td>\n",
       "      <td>1213.935318</td>\n",
       "      <td>535.429241</td>\n",
       "      <td>1212.952541</td>\n",
       "      <td>531.396316</td>\n",
       "      <td>1211.632288</td>\n",
       "      <td>527.412675</td>\n",
       "      <td>1209.989101</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3200 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID           v1           v2           v3           v4           v5  \\\n",
       "0     10002  1719.409995   336.369493  1725.203314   336.284549  1731.180275   \n",
       "1     10015   727.496177  1228.487343   729.501623  1227.020683   731.551209   \n",
       "2     10019   575.937420  1245.525411   577.949851  1246.229434   579.888580   \n",
       "3     10028  1688.421868   315.642609  1686.011086   315.433464  1683.885100   \n",
       "4      1003  2124.988001   677.337634  2126.253720   676.509473  2127.818240   \n",
       "...     ...          ...          ...          ...          ...          ...   \n",
       "3195   9897   256.107934   806.429025   256.087218   807.626338   256.094916   \n",
       "3196     99   588.224553  1155.657332   588.625253  1156.738643   589.040805   \n",
       "3197   9905  1759.557891   444.402348  1763.731861   444.406015  1767.959124   \n",
       "3198   9910   576.237940  1291.916656   577.363511  1294.875644   578.157823   \n",
       "3199   9918   587.638949  1166.481728   589.380660  1168.105146   590.439682   \n",
       "\n",
       "               v6           v7           v8           v9  ...          v51  \\\n",
       "0      336.088659  1737.284381   335.811184  1743.468707  ...  1852.029550   \n",
       "1     1225.541131   733.649858  1224.036116   735.807510  ...   787.935138   \n",
       "2     1246.909017   581.766447  1247.556617   583.588199  ...   618.134380   \n",
       "3      315.277506  1682.060234   315.193592  1680.518450  ...  1658.053798   \n",
       "4      675.686718  2129.642917   674.856144  2131.708043  ...  2218.717961   \n",
       "...           ...          ...          ...          ...  ...          ...   \n",
       "3195   809.096276   256.088205   810.722548   256.036828  ...   240.068192   \n",
       "3196  1157.748837   589.466163  1158.689328   589.894316  ...   595.922750   \n",
       "3197   444.195250  1772.224438   443.849696  1776.542309  ...  1866.443332   \n",
       "3198  1297.933344   578.671536  1301.083615   578.939946  ...   541.070220   \n",
       "3199  1170.143350   591.029620  1172.422023   591.255681  ...   543.506071   \n",
       "\n",
       "              v52          v53          v54          v55          v56  \\\n",
       "0      325.809922  1855.185078   325.509085  1858.136674   325.225742   \n",
       "1     1192.248323   789.823385  1191.082460   791.612788  1189.971742   \n",
       "2     1208.988457   619.759209  1205.888133   621.359993  1202.839988   \n",
       "3      317.807724  1656.362865   318.090679  1654.566922   318.396584   \n",
       "4      644.154731  2224.051100   642.274036  2229.361461   640.417128   \n",
       "...           ...          ...          ...          ...          ...   \n",
       "3195   856.978647   238.445174   858.486782   236.737045   859.748825   \n",
       "3196  1171.919051   596.053556  1172.410363   596.170164  1172.903513   \n",
       "3197   422.076754  1869.704598   421.300012  1872.801255   420.583009   \n",
       "3198  1382.560169   537.053684  1384.583994   532.847029  1386.130476   \n",
       "3199  1214.570876   539.476751  1213.935318   535.429241  1212.952541   \n",
       "\n",
       "              v57          v58          v59          v60  \n",
       "0     1860.890789   324.958520  1863.455956   324.706589  \n",
       "1      793.303437  1188.914976   794.896137  1187.909617  \n",
       "2      622.930811  1199.866418   624.466382  1196.988248  \n",
       "3     1652.656799   318.726605  1650.622822   319.082017  \n",
       "4     2234.624730   638.597277  2239.818130   636.827166  \n",
       "...           ...          ...          ...          ...  \n",
       "3195   234.947427   860.738632   233.081285   861.434141  \n",
       "3196   596.272536  1173.400056   596.360345  1173.901477  \n",
       "3197  1875.730781   419.922832  1878.491704   419.315732  \n",
       "3198   528.465023  1387.168444   523.925801  1387.676014  \n",
       "3199   531.396316  1211.632288   527.412675  1209.989101  \n",
       "\n",
       "[3200 rows x 61 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db08b55e-05e6-409e-b831-5f23ccd42ab3",
   "metadata": {
    "id": "db08b55e-05e6-409e-b831-5f23ccd42ab3"
   },
   "outputs": [],
   "source": [
    "# Ensemble Method "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
